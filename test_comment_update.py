#!/usr/bin/env python3
"""
Test script for the improved comment management system.
"""

import tempfile
import os
from scripts.manage_pr_comments import PRCommentManager

def test_comment_detection():
    """Test that the comment manager can detect all types of benchmark comments."""
    
    # Mock comment manager (without actual API calls)
    manager = PRCommentManager("fake-token", "test-owner", "test-repo")
    
    # Test comments with different signatures
    test_comments = [
        {"id": 1, "body": "Some regular comment"},
        {"id": 2, "body": "<!-- datason-benchmark-bot -->\n🚀 DataSON PR Performance Analysis"},
        {"id": 3, "body": "❌ Performance Regression Detected\n\nSome details..."},
        {"id": 4, "body": "Generated by datason-benchmarks • Comprehensive Performance Analysis"},
        {"id": 5, "body": "Regular comment with Generated by [datason-benchmarks] in the middle"},
        {"id": 6, "body": "Another regular comment"},
    ]
    
    # Test signature detection
    benchmark_comments = []
    for comment in test_comments:
        comment_body = comment.get('body', '')
        if any(signature in comment_body for signature in manager.comment_signatures):
            benchmark_comments.append(comment)
    
    print("🔍 Comment Detection Test Results:")
    print(f"Total comments: {len(test_comments)}")
    print(f"Detected benchmark comments: {len(benchmark_comments)}")
    print(f"Expected: 4 benchmark comments (IDs: 2, 3, 4, 5)")
    
    detected_ids = [c['id'] for c in benchmark_comments]
    expected_ids = [2, 3, 4, 5]
    
    if detected_ids == expected_ids:
        print("✅ Comment detection working correctly!")
        return True
    else:
        print(f"❌ Comment detection failed. Got IDs: {detected_ids}")
        return False

def test_comment_strategies():
    """Test the different comment update strategies."""
    
    print("\n🧪 Testing Comment Update Strategies:")
    
    # Test strategy descriptions
    strategies = {
        "update": "Updates the most recent benchmark comment",
        "mark_outdated": "Marks old comments as outdated, creates new one",
        "replace_all": "Deletes old comments, creates new one"
    }
    
    for strategy, description in strategies.items():
        print(f"✅ {strategy}: {description}")
    
    print("✅ All strategies properly documented!")
    return True

def test_final_comment_format():
    """Test the final comment format that will be generated."""
    
    print("\n📝 Testing Final Comment Format:")
    
    # Create test files
    with tempfile.TemporaryDirectory() as temp_dir:
        os.chdir(temp_dir)
        
        # Create test regression comment
        with open('pr_regression_comment.md', 'w') as f:
            f.write("""❌ **Performance Regression Detected**

### ❌ Critical Regressions (PR should not be merged)
| Metric | Baseline | Current | Change |
|--------|----------|---------|--------|
| serialize_time | 100μs | 150μs | **+50%** |

### 🚀 Performance Improvements  
| Metric | Baseline | Current | Change |
|--------|----------|---------|--------|
| deserialize_time | 10ms | 1ms | **-90%** |""")
        
        # Create test main comment
        with open('pr_comment.md', 'w') as f:
            f.write("""📊 **Benchmark Results**
Suite: pr_optimized | Tests Run: 5 | Success Rate: 100.0%

🎯 **DataSON Performance Summary**
All tests completed successfully with significant improvements in deserialization.""")
        
        # Simulate the workflow's comment generation
        with open('final_pr_comment.md', 'w') as f:
            f.write("""<!-- datason-benchmark-bot -->
🚀 **DataSON PR Performance Analysis**
PR #123 | Commit: abc123def456

""")
            
            # Add regression analysis
            if os.path.exists('pr_regression_comment.md'):
                with open('pr_regression_comment.md', 'r') as reg_f:
                    f.write(reg_f.read())
                f.write("\n\n---\n\n")
            
            # Add main benchmark results
            if os.path.exists('pr_comment.md'):
                with open('pr_comment.md', 'r') as main_f:
                    f.write(main_f.read())
                f.write("\n\n")
            
            # Add footer
            f.write("""---
*Generated by datason-benchmarks • Comprehensive Performance Analysis*""")
        
        # Read and display the final comment
        with open('final_pr_comment.md', 'r') as f:
            final_comment = f.read()
        
        print("Final comment structure:")
        print("=" * 50)
        print(final_comment)
        print("=" * 50)
        
        # Check that it contains all expected parts
        checks = [
            ("Bot signature", "datason-benchmark-bot" in final_comment),
            ("Header", "DataSON PR Performance Analysis" in final_comment),
            ("PR info", "PR #123" in final_comment),
            ("Regression data", "Performance Regression Detected" in final_comment),
            ("Main results", "Benchmark Results" in final_comment),
            ("Footer", "Generated by datason-benchmarks" in final_comment),
            ("No duplicated content", final_comment.count("Performance Regression Detected") == 1)
        ]
        
        all_passed = True
        for check_name, passed in checks:
            status = "✅" if passed else "❌"
            print(f"{status} {check_name}")
            if not passed:
                all_passed = False
        
        return all_passed

if __name__ == "__main__":
    print("🧪 Testing Enhanced Comment Management System")
    print("=" * 60)
    
    tests_passed = 0
    total_tests = 3
    
    if test_comment_detection():
        tests_passed += 1
    
    if test_comment_strategies():
        tests_passed += 1
        
    if test_final_comment_format():
        tests_passed += 1
    
    print(f"\n📊 Test Results: {tests_passed}/{total_tests} tests passed")
    
    if tests_passed == total_tests:
        print("🎉 All tests passed! Comment management system is ready.")
    else:
        print("⚠️ Some tests failed. Please review the implementation.")