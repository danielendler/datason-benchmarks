name: ğŸ“Š Daily Benchmarks

on:
  schedule:
    # Run every day at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - 'quick'
          - 'competitive'
          - 'configurations'
          - 'versioning'
          - 'all'

permissions:
  contents: write
  pages: write
  id-token: write

env:
  PYTHONUNBUFFERED: 1
  CI: true
  GITHUB_ACTIONS: true

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0

    - name: ğŸ Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: ğŸ’¾ Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pypoetry
        key: benchmark-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          benchmark-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-
          benchmark-deps-${{ runner.os }}-py3.11-

    - name: ğŸ’¾ Cache competitor libraries
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip/http
          ~/.cache/pip/wheels
        key: competitors-${{ runner.os }}-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          competitors-${{ runner.os }}-${{ hashFiles('requirements.txt') }}-
          competitors-${{ runner.os }}-

    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
        # Verify all competitive libraries are available
        python -c "
        import sys
        try:
            import datason, orjson, ujson, json, pickle, jsonpickle, msgpack
            print('âœ… All competitive libraries installed successfully')
        except ImportError as e:
            print(f'âŒ Missing library: {e}')
            sys.exit(1)
        "

    - name: ğŸ“Š Run CI benchmark suite
      run: |
        BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'all' }}"
        echo "ğŸš€ Running CI benchmark type: $BENCHMARK_TYPE"
        
        # Ensure results directory exists
        mkdir -p data/results docs/results
        
        case $BENCHMARK_TYPE in
          "quick")
            python scripts/run_benchmarks.py --quick --generate-report
            ;;
          "competitive")
            python scripts/run_benchmarks.py --competitive --generate-report
            ;;
          "configurations")
            python scripts/run_benchmarks.py --configurations --generate-report
            ;;
          "versioning")
            python scripts/run_benchmarks.py --versioning --generate-report
            ;;
          "all")
            python scripts/run_benchmarks.py --all --generate-report
            ;;
          *)
            echo "Unknown benchmark type: $BENCHMARK_TYPE, running all"
            python scripts/run_benchmarks.py --all --generate-report
            ;;
        esac
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}

    - name: ğŸ·ï¸ Tag results with CI metadata
      run: |
        # Create CI-tagged copies of all generated results
        timestamp=$(date -u '+%Y%m%d_%H%M%S')
        run_id="${{ github.run_id }}"
        
        cd data/results
        
        # Tag all latest_*.json files with CI prefix
        for file in latest_*.json; do
          if [ -f "$file" ]; then
            suite_type=$(echo "$file" | sed 's/latest_\(.*\)\.json/\1/')
            ci_filename="ci_${timestamp}_${run_id}_${suite_type}.json"
            cp "$file" "$ci_filename"
            echo "âœ… Created CI result: $ci_filename"
          fi
        done

    - name: ğŸ“Š Generate GitHub Pages index
      run: |
        python scripts/generate_github_pages.py
        
        # Also update general docs index for navigation
        python scripts/update_docs_index.py
      env:
        GITHUB_REPOSITORY: ${{ github.repository }}

    - name: ğŸŒ Commit CI results to repository
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Force add CI-tagged results (override gitignore)
        git add -f data/results/ci_*_*.json
        git add -f docs/results/ci_*_*.html
        git add docs/results/index.html
        git add docs/index.md
        
        # Commit if there are changes
        if ! git diff --staged --quiet; then
          commit_msg="ğŸ“Š CI Benchmark Results - $(date -u '+%Y-%m-%d %H:%M UTC')

        Suite: ${{ github.event.inputs.benchmark_type || 'all' }}
        Run ID: ${{ github.run_id }}
        SHA: ${{ github.sha }}"
          
          git commit -m "$commit_msg"
          git push
          echo "âœ… CI results committed to repository"
        else
          echo "â„¹ï¸ No changes to commit"
        fi

    - name: ğŸ“¤ Upload artifacts for analysis
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: daily-benchmark-results-${{ github.run_id }}
        path: |
          data/results/ci_*_*.json
          docs/results/ci_*_*.html
          docs/results/index.html
        retention-days: 90

    - name: âœ… Benchmark complete
      run: |
        echo "ğŸ‰ Daily benchmark suite completed successfully!"
        echo "ğŸ“Š Results available in repository and GitHub Pages"
        echo "ğŸ”— https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" 