name: 🧪 CI Tests & Validation

on:
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/**'
      - 'scripts/**'
      - 'tests/**'
      - 'requirements.txt'
      - '*.py'
  push:
    branches: [main]
    paths:
      - '.github/workflows/**'
      - 'scripts/**'
      - 'tests/**'
      - 'requirements.txt'
      - '*.py'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: 1
  CI: true

jobs:
  workflow-validation:
    name: 🔧 Workflow Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: 📦 Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pyyaml

    - name: 🧪 Run workflow integration tests
      run: |
        echo "🔍 Running comprehensive workflow validation tests..."
        python -m pytest tests/test_workflow_integration.py -v --tb=short
        
    - name: 📊 Validate YAML syntax
      run: |
        echo "🔍 Validating all workflow YAML files..."
        python -c "
        import yaml
        import glob
        import sys
        
        failed = False
        for workflow_file in glob.glob('.github/workflows/*.yml'):
            try:
                with open(workflow_file, 'r') as f:
                    yaml.safe_load(f)
                print(f'✅ {workflow_file}')
            except yaml.YAMLError as e:
                print(f'❌ {workflow_file}: {e}')
                failed = True
        
        if failed:
            sys.exit(1)
        print('🎉 All workflow YAML files are valid')
        "

  benchmarks-module-tests:
    name: 🧮 Benchmarks Module Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: 📦 Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pyyaml datason pandas numpy matplotlib plotly

    - name: 🧪 Run benchmarks module tests
      run: |
        echo "🔍 Running datason-benchmarks module tests..."
        # Run any existing tests in the module
        if [ -f tests/test_benchmarks_module.py ]; then
          python -m pytest tests/test_benchmarks_module.py -v --tb=short
        else
          echo "📝 Module tests not found - will be created"
        fi

    - name: 🔗 Test symlink logic (if applicable)
      run: |
        echo "🔍 Testing latest results symlink logic..."
        # Test the symlink creation and management
        mkdir -p data/results
        echo '{"test": "data"}' > data/results/test_baseline.json
        
        # Test symlink creation
        cd data/results
        ln -sf test_baseline.json latest.json
        
        # Verify symlink works
        if [ -L latest.json ] && [ -e latest.json ]; then
          echo "✅ Symlink functionality working"
        else
          echo "❌ Symlink functionality broken"
          exit 1
        fi

    - name: 🧮 Test unit conversion utilities
      run: |
        echo "🔍 Testing unit conversion utilities..."
        python -c "
        import sys
        import os
        sys.path.insert(0, 'scripts')
        
        # Test if unit conversion utilities exist and work
        try:
            # Look for unit conversion functions in scripts
            import glob
            conversion_found = False
            
            for script in glob.glob('scripts/*.py'):
                with open(script, 'r') as f:
                    content = f.read()
                    if any(term in content.lower() for term in ['microsecond', 'millisecond', 'format_time', 'unit', 'convert']):
                        conversion_found = True
                        print(f'📊 Found potential unit conversion logic in {script}')
            
            if conversion_found:
                print('✅ Unit conversion utilities detected')
            else:
                print('📝 No unit conversion utilities found - may need implementation')
                
        except Exception as e:
            print(f'⚠️ Error testing unit conversion: {e}')
        "

  integration-check:
    name: 🔗 Integration Check
    runs-on: ubuntu-latest
    needs: [workflow-validation, benchmarks-module-tests]
    if: always()

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 📊 Check integration health
      run: |
        echo "🔍 Checking overall integration health..."
        
        # Check if critical files exist
        CRITICAL_FILES=(
          ".github/workflows/pr-performance-check.yml"
          ".github/workflows/daily-benchmarks.yml"
          "scripts/run_benchmarks.py"
          "tests/test_workflow_integration.py"
        )
        
        for file in "${CRITICAL_FILES[@]}"; do
          if [ -f "$file" ]; then
            echo "✅ $file exists"
          else
            echo "❌ $file missing"
            exit 1
          fi
        done
        
        echo "🎉 Integration health check passed"

    - name: 📝 Generate test summary
      run: |
        echo "## 🧪 CI Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Workflow Validation:** ${{ needs.workflow-validation.result }}" >> $GITHUB_STEP_SUMMARY
        echo "**Module Tests:** ${{ needs.benchmarks-module-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Key Validations" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ YAML syntax validation" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Workflow integration tests" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Benchmarks module tests" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Symlink logic verification" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Unit conversion utilities check" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "*This ensures datason-benchmarks repository integrity*" >> $GITHUB_STEP_SUMMARY