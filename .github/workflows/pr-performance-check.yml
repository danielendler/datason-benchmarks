name: 🚀 PR Performance Check

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - 'benchmarks/**'
      - 'competitors/**'
      - 'requirements.txt'
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'External PR number'
        required: false
        type: string
      commit_sha:
        description: 'External commit SHA'
        required: false
        type: string
      artifact_name:
        description: 'External wheel artifact name'
        required: false
        type: string
      datason_repo:
        description: 'External DataSON repo (owner/repo)'
        required: false
        type: string
      benchmark_type:
        description: 'Benchmark type'
        required: false
        default: 'pr_optimized'
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: 1
  CI: true
  GITHUB_ACTIONS: true

jobs:
  quick-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: 💾 Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-
          pr-deps-${{ runner.os }}-py3.11-
          benchmark-deps-${{ runner.os }}-py3.11-

    - name: 📥 Download external DataSON wheel (if triggered externally)
      if: github.event.inputs.datason_repo != '' && github.event.inputs.artifact_name != ''
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.BENCHMARK_REPO_TOKEN }}
        script: |
          const fs = require('fs');
          
          // Parse external repository info
          const [owner, repo] = '${{ github.event.inputs.datason_repo }}'.split('/');
          const artifactName = '${{ github.event.inputs.artifact_name }}';
          const commitSha = '${{ github.event.inputs.commit_sha }}';
          
          console.log(`🔍 Searching for external artifact: ${artifactName}`);
          console.log(`📦 Repository: ${owner}/${repo}`);
          console.log(`🔗 Commit: ${commitSha}`);
          
          // Get workflow runs for the commit
          const runsResponse = await github.rest.actions.listWorkflowRunsForRepo({
            owner: owner,
            repo: repo,
            head_sha: commitSha,
            status: 'completed',
            per_page: 20
          });
          
          // Find the artifact from the most recent successful run
          let artifactId = null;
          for (const run of runsResponse.data.workflow_runs) {
            if (run.conclusion === 'success') {
              try {
                const artifactsResponse = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: owner,
                  repo: repo,
                  run_id: run.id
                });
                
                const artifact = artifactsResponse.data.artifacts.find(a => a.name === artifactName);
                if (artifact && !artifact.expired) {
                  console.log(`✅ Found external artifact: ${artifact.name}`);
                  artifactId = artifact.id;
                  break;
                }
              } catch (error) {
                console.log(`⚠️ Could not access artifacts for run ${run.id}`);
              }
            }
          }
          
          if (!artifactId) {
            console.log(`⚠️ Could not find external artifact '${artifactName}' - using PyPI version`);
            return;
          }
          
          // Download the external artifact
          console.log('📥 Downloading external DataSON wheel...');
          const download = await github.rest.actions.downloadArtifact({
            owner: owner,
            repo: repo,
            artifact_id: artifactId,
            archive_format: 'zip'
          });
          
          // Save the artifact
          fs.mkdirSync('external-wheel', { recursive: true });
          fs.writeFileSync('external-wheel/datason-wheel.zip', Buffer.from(download.data));
          console.log('✅ External DataSON wheel downloaded');

    - name: 📦 Install dependencies
      run: |
        python -m pip install --upgrade pip
        
        # Install external DataSON wheel if available
        if [ -f external-wheel/datason-wheel.zip ]; then
          echo "🔧 Installing external DataSON wheel..."
          cd external-wheel
          unzip -q datason-wheel.zip
          WHEEL_FILE=$(find . -name "*.whl" -type f | head -n1)
          if [ -n "$WHEEL_FILE" ]; then
            echo "📦 Installing: $WHEEL_FILE"
            pip install "$WHEEL_FILE"
            python -c "import datason; print(f'✅ External DataSON {datason.__version__} installed')"
          else
            echo "⚠️ No wheel file found in external artifact"
            pip install -r requirements.txt
          fi
          cd ..
        else
          echo "📦 Installing DataSON from requirements.txt"
          pip install -r requirements.txt
        fi
        
        # Install remaining dependencies
        pip install orjson ujson msgpack jsonpickle numpy pandas matplotlib plotly requests
        
        # Verify all competitive libraries are available
        python -c "
        import sys
        try:
            import datason, orjson, ujson, json, pickle, jsonpickle, msgpack
            print('✅ All competitive libraries installed successfully')
            print(f'📊 DataSON version: {datason.__version__}')
        except ImportError as e:
            print(f'❌ Missing library: {e}')
            sys.exit(1)
        "

    - name: 🚀 Run PR benchmark suite
      run: |
        echo "🚀 Running PR performance check..."
        
        # Ensure results directory exists
        mkdir -p data/results docs/results
        
        # Run quick benchmark with report generation
        python scripts/run_benchmarks.py --quick --generate-report

        # Run Rust-core micro-benchmark
        DATASON_RUST=1 python scripts/bench_rust_core.py save_string --sizes 10k --shapes flat --repeat 1 --output rust_core_result.json

    - name: 📦 Upload Rust-core results
      uses: actions/upload-artifact@v4
      with:
        name: rust-core-results
        path: rust_core_result.json
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}

    - name: 📊 Generate PR comment
      if: github.event_name == 'pull_request' || github.event.inputs.pr_number != ''
      run: |
        # Determine PR number and commit SHA for both internal and external triggers
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          PR_NUMBER="${{ github.event.number }}"
          COMMIT_SHA="${{ github.sha }}"
          BENCHMARK_TYPE="quick"
        else
          PR_NUMBER="${{ github.event.inputs.pr_number }}"
          COMMIT_SHA="${{ github.event.inputs.commit_sha }}"
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'pr_optimized' }}"
        fi
        
        if [ -f data/results/latest_quick.json ]; then
          python scripts/generate_pr_comment.py \
            --pr-number "$PR_NUMBER" \
            --commit-sha "$COMMIT_SHA" \
            --benchmark-type "$BENCHMARK_TYPE" \
            --result-file data/results/latest_quick.json \
            --baseline-file data/results/latest.json \
            --output pr_comment.md

          cat pr_comment.md >> "$GITHUB_STEP_SUMMARY"
          echo "📊 PR comment generated"
        else
          echo "⚠️ No benchmark results found" | tee -a "$GITHUB_STEP_SUMMARY"
          echo "No benchmark results found" > pr_comment.md
        fi
    - name: 🔍 Advanced Regression Detection
      run: |
        # Install regression detection dependencies if needed
        pip install pandas matplotlib 2>/dev/null || true
        
        # Find actual benchmark result files
        RESULT_FILES=$(find data/results -name "latest_*.json" -not -name "latest.json" 2>/dev/null | head -5)
        
        if [ -f data/results/latest.json ] && [ -n "$RESULT_FILES" ]; then
          echo "📊 Running advanced regression detection..."
          echo "Found result files: $RESULT_FILES"
          
          python scripts/regression_detector.py \
            $RESULT_FILES \
            --baseline data/results/latest.json \
            --output data/results/pr_regression_report.json \
            --pr-comment pr_regression_comment.md \
            --fail-threshold 0.25 \
            --warn-threshold 0.10
          
          REGRESSION_EXIT_CODE=$?
          echo "REGRESSION_EXIT_CODE=$REGRESSION_EXIT_CODE" >> $GITHUB_ENV
          
          if [ $REGRESSION_EXIT_CODE -ne 0 ]; then
            echo "⚠️ Critical performance regressions detected!"
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          else
            echo "✅ No critical regressions detected"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi
          
        else
          echo "📝 No baseline or benchmark results found - creating baseline comment"
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "🔄 **Performance Baseline**" > pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "This is the first benchmark run or no previous baseline was found." >> pr_regression_comment.md
          echo "Future PRs will be compared against this baseline." >> pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "**Available Results:**" >> pr_regression_comment.md
          if [ -n "$RESULT_FILES" ]; then
            for file in $RESULT_FILES; do
              echo "- $(basename $file)" >> pr_regression_comment.md
            done
          else
            echo "- No benchmark result files found" >> pr_regression_comment.md
          fi
        fi



    - name: 💬 Update PR comment
      if: github.event_name == 'pull_request' && hashFiles('pr_comment.md') != ''
      uses: actions/github-script@v7
      continue-on-error: true
      with:
        script: |
          const fs = require('fs');

          try {
            // Priority: Use regression comment if available, fallback to main comment
            let comment = '';

            if (fs.existsSync('pr_regression_comment.md')) {
              const regressionComment = fs.readFileSync('pr_regression_comment.md', 'utf8');

              if (fs.existsSync('pr_comment.md')) {
                const mainComment = fs.readFileSync('pr_comment.md', 'utf8');
                comment = regressionComment + '\n---\n' + mainComment;
              } else {
                comment = regressionComment;
              }
            } else if (fs.existsSync('pr_comment.md')) {
              comment = fs.readFileSync('pr_comment.md', 'utf8');
            }

            if (!comment) {
              console.log('No comment content found');
              return;
            }

            // Add regression warning to top if needed
            const hasRegression = process.env.PERFORMANCE_REGRESSION === 'true';
            if (hasRegression) {
              comment = '🚨 **PERFORMANCE REGRESSION DETECTED**\n\n' + comment;
            }

            // Add header to identify bot comments
            const botHeader = '<!-- datason-benchmark-bot -->\n';
            comment = botHeader + comment;

            // Determine target repository for comment posting
            const isExternalTrigger = '${{ github.event.inputs.datason_repo }}' !== '';
            const prNumber = '${{ github.event.inputs.pr_number }}' || context.issue.number;
            
            let targetOwner, targetRepo;
            if (isExternalTrigger && '${{ github.event.inputs.datason_repo }}') {
              [targetOwner, targetRepo] = '${{ github.event.inputs.datason_repo }}'.split('/');
            } else {
              targetOwner = context.repo.owner;
              targetRepo = context.repo.repo;
            }

            // Look for existing bot comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: targetOwner,
              repo: targetRepo,
              issue_number: prNumber,
            });

            const existingComment = comments.find(c => 
              c.body.includes('<!-- datason-benchmark-bot -->') && 
              c.user.type === 'Bot'
            );

            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: targetOwner,
                repo: targetRepo,
                comment_id: existingComment.id,
                body: comment,
              });
              console.log(`Updated existing comment #${existingComment.id} in ${targetOwner}/${targetRepo}#${prNumber}`);
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: targetOwner,
                repo: targetRepo,
                issue_number: prNumber,
                body: comment,
              });
              console.log(`Created new comment in ${targetOwner}/${targetRepo}#${prNumber}`);
            }
          } catch (error) {
            console.log(`Failed to update comment: ${error.message}`);
          }

    - name: 📤 Upload enhanced artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
          name: pr-performance-check-${{ github.run_id }}
          path: |
            data/results/latest_*.json
            data/results/*_benchmark_*.json
            data/results/pr_regression_report.json
            docs/results/*_report.html
            pr_comment.md
            pr_regression_comment.md
          retention-days: 30

    - name: ✅ Performance check complete
      run: |
        echo "✅ PR performance check completed"
        echo "📊 Interactive report and detailed analysis available in artifacts"
        echo "💬 PR comment posted with performance analysis" 
