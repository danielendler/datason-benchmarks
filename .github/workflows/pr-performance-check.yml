name: ðŸš€ PR Performance Check

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - 'benchmarks/**'
      - 'competitors/**'
      - 'requirements.txt'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: 1
  CI: true
  GITHUB_ACTIONS: true

jobs:
  quick-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: ðŸ’¾ Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pypoetry
        key: pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-
          pr-deps-${{ runner.os }}-py3.11-
          benchmark-deps-${{ runner.os }}-py3.11-

    - name: ðŸ’¾ Cache competitor libraries
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip/http
          ~/.cache/pip/wheels
        key: pr-competitors-${{ runner.os }}-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          pr-competitors-${{ runner.os }}-${{ hashFiles('requirements.txt') }}-
          pr-competitors-${{ runner.os }}-
          competitors-${{ runner.os }}-

    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
        # Verify all competitive libraries are available
        python -c "
        import sys
        try:
            import datason, orjson, ujson, json, pickle, jsonpickle, msgpack
            print('âœ… All competitive libraries installed successfully')
        except ImportError as e:
            print(f'âŒ Missing library: {e}')
            sys.exit(1)
        "

    - name: ðŸš€ Run PR benchmark suite
      run: |
        echo "ðŸš€ Running PR performance check..."
        
        # Ensure results directory exists
        mkdir -p data/results docs/results
        
        # Run quick benchmark with report generation
        python scripts/run_benchmarks.py --quick --generate-report
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}

    - name: ðŸ“Š Generate enhanced PR comment
      if: github.event_name == 'pull_request'
      run: |
        python -c "
        import json
        import os
        from pathlib import Path

        # Look for latest results
        results_dir = Path('data/results')
        latest_file = results_dir / 'latest_quick.json'

        if latest_file.exists():
            with open(latest_file, 'r') as f:
                results = json.load(f)

            # Generate enhanced markdown report
            report = []
            report.append('# ðŸš€ PR Performance Check Results')
            report.append('')
            report.append('> **CI Environment**: This benchmark was run in GitHub Actions for consistent results.')
            report.append('')
            
            # Metadata
            metadata = results.get('metadata', {})
            report.append('## ðŸ“‹ Test Environment')
            report.append('')
            report.append(f'- **Run ID**: {os.environ.get(\"GITHUB_RUN_ID\", \"unknown\")}')
            report.append(f'- **Commit**: {os.environ.get(\"GITHUB_SHA\", \"unknown\")[:8]}')
            report.append(f'- **Python**: {metadata.get(\"python_version\", \"unknown\").split()[0]}')
            report.append('')

            if 'competitive' in results:
                competitive = results['competitive']
                
                # Performance regression check
                report.append('## ðŸŽ¯ Performance Analysis')
                report.append('')
                
                regression_found = False
                good_performance = []
                concerning_performance = []
                
                for dataset, data in competitive.items():
                    if dataset == 'summary':
                        continue
                    
                    ser_results = data.get('serialization', {})
                    if 'datason' in ser_results:
                        datason_time = ser_results['datason'].get('mean_ms', 0)
                        
                        # Find fastest competitor
                        fastest_time = float('inf')
                        fastest_lib = 'none'
                        for lib, metrics in ser_results.items():
                            if lib != 'datason' and isinstance(metrics, dict):
                                time_ms = metrics.get('mean_ms', float('inf'))
                                if time_ms < fastest_time:
                                    fastest_time = time_ms
                                    fastest_lib = lib
                        
                        ratio = datason_time / fastest_time if fastest_time != float('inf') else 1.0
                        
                        if ratio > 5.0:
                            concerning_performance.append((dataset, datason_time, fastest_lib, fastest_time, ratio))
                            regression_found = True
                        elif ratio < 2.0:
                            good_performance.append((dataset, datason_time, fastest_lib, fastest_time, ratio))
                
                # Summary verdict
                if regression_found:
                    report.append('### âš ï¸ Performance Concerns Detected')
                    report.append('')
                    for dataset, ds_time, fast_lib, fast_time, ratio in concerning_performance:
                        report.append(f'- **{dataset}**: DataSON ({ds_time:.2f}ms) is {ratio:.2f}x slower than {fast_lib} ({fast_time:.2f}ms)')
                    report.append('')
                else:
                    report.append('### âœ… Performance Within Expected Range')
                    report.append('')
                    report.append('No significant performance regressions detected.')
                    report.append('')
                
                # Summary table
                report.append('## ðŸ“Š Detailed Performance Comparison')
                report.append('')
                report.append('| Dataset | DataSON (ms) | Fastest Competitor | Performance Ratio | Status |')
                report.append('|---------|--------------|-------------------|------------------|--------|')
                
                for dataset, data in competitive.items():
                    if dataset == 'summary':
                        continue
                    
                    ser_results = data.get('serialization', {})
                    if 'datason' in ser_results:
                        datason_time = ser_results['datason'].get('mean_ms', 0)
                        
                        # Find fastest competitor
                        fastest_time = float('inf')
                        fastest_lib = 'none'
                        for lib, metrics in ser_results.items():
                            if lib != 'datason' and isinstance(metrics, dict):
                                time_ms = metrics.get('mean_ms', float('inf'))
                                if time_ms < fastest_time:
                                    fastest_time = time_ms
                                    fastest_lib = lib
                        
                        ratio = datason_time / fastest_time if fastest_time != float('inf') else 1.0
                        
                        if ratio < 1.5:
                            status = 'ðŸš€ Excellent'
                        elif ratio < 2.0:
                            status = 'âœ… Good'
                        elif ratio < 5.0:
                            status = 'âš ï¸ Acceptable'
                        else:
                            status = 'âŒ Concerning'
                        
                        report.append(f'| {dataset} | {datason_time:.3f} | {fastest_lib} ({fastest_time:.3f}) | {ratio:.2f}x | {status} |')
                
                report.append('')
                report.append('**Performance Guide**:')
                report.append('- ðŸš€ **Excellent**: <1.5x slower than fastest')
                report.append('- âœ… **Good**: 1.5-2x slower')  
                report.append('- âš ï¸ **Acceptable**: 2-5x slower')
                report.append('- âŒ **Concerning**: >5x slower')
                report.append('')
                
                # Interactive report link
                report.append('## ðŸ“ˆ Interactive Report')
                report.append('')
                report.append('An interactive HTML report with charts has been generated. ')
                report.append('Download the artifacts to view detailed performance visualizations.')
                report.append('')
                
                # Detailed results
                report.append('<details>')
                report.append('<summary>ðŸ“‹ Raw Performance Data</summary>')
                report.append('')
                
                for dataset, data in competitive.items():
                    if dataset == 'summary':
                        continue
                    
                    report.append(f'### {dataset}')
                    if 'description' in data:
                        report.append(f'*{data[\"description\"]}*')
                    report.append('')
                    report.append('| Library | Mean (ms) | Min (ms) | Max (ms) | Std Dev (ms) | Success Rate |')
                    report.append('|---------|-----------|----------|----------|--------------|--------------|')
                    
                    ser_results = data.get('serialization', {})
                    for lib, metrics in ser_results.items():
                        if isinstance(metrics, dict):
                            mean_ms = metrics.get('mean_ms', 0)
                            min_ms = metrics.get('min_ms', 0)
                            max_ms = metrics.get('max_ms', 0)
                            std_ms = metrics.get('std_ms', 0)
                            success = metrics.get('successful_runs', 0)
                            total = success + metrics.get('error_count', 0)
                            rate = f'{success}/{total}' if total > 0 else 'N/A'
                            
                            report.append(f'| {lib} | {mean_ms:.3f} | {min_ms:.3f} | {max_ms:.3f} | {std_ms:.3f} | {rate} |')
                    
                    report.append('')
                
                report.append('</details>')

            # Save to GitHub step summary
            with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
                f.write('\\n'.join(report))

            # Save for PR comment
            with open('pr_comment.md', 'w') as f:
                f.write('\\n'.join(report))
                
            print('ðŸ“Š Generated enhanced PR comment')
        else:
            print('âš ï¸ No benchmark results found')
        "
    - name: ðŸ” Advanced Regression Detection
      run: |
        # Install regression detection dependencies if needed
        pip install pandas matplotlib 2>/dev/null || true
        
        # Run comprehensive regression detection
        if [ -f data/results/latest.json ]; then
          echo "ðŸ“Š Running advanced regression detection..."
          python scripts/regression_detector.py \
            data/results/latest_competitive_*.json \
            --baseline data/results/latest.json \
            --output data/results/pr_regression_report.json \
            --pr-comment pr_regression_comment.md \
            --fail-threshold 0.25 \
            --warn-threshold 0.10
          
          REGRESSION_EXIT_CODE=$?
          echo "REGRESSION_EXIT_CODE=$REGRESSION_EXIT_CODE" >> $GITHUB_ENV
          
          if [ $REGRESSION_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Critical performance regressions detected!"
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          else
            echo "âœ… No critical regressions detected"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi
          
        else
          echo "ðŸ“ No baseline found - this will become the new baseline"
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "ðŸ”„ **Performance Baseline**" > pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "This is the first benchmark run or no previous results were found." >> pr_regression_comment.md
          echo "Future PRs will be compared against this baseline." >> pr_regression_comment.md
        fi
    - name: ðŸ” Advanced Regression Detection
      run: |
        # Install regression detection dependencies if needed
        pip install pandas matplotlib 2>/dev/null || true
        
        # Run comprehensive regression detection
        if [ -f data/results/latest.json ]; then
          echo "ðŸ“Š Running advanced regression detection..."
          python scripts/regression_detector.py \
            data/results/latest_competitive_*.json \
            --baseline data/results/latest.json \
            --output data/results/pr_regression_report.json \
            --pr-comment pr_regression_comment.md \
            --fail-threshold 0.25 \
            --warn-threshold 0.10
          
          REGRESSION_EXIT_CODE=$?
          echo "REGRESSION_EXIT_CODE=$REGRESSION_EXIT_CODE" >> $GITHUB_ENV
          
          if [ $REGRESSION_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Critical performance regressions detected!"
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          else
            echo "âœ… No critical regressions detected"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi
          
        else
          echo "ðŸ“ No baseline found - this will become the new baseline"
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "ðŸ”„ **Performance Baseline**" > pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "This is the first benchmark run or no previous results were found." >> pr_regression_comment.md
          echo "Future PRs will be compared against this baseline." >> pr_regression_comment.md
        fi



    - name: ðŸ’¬ Comment on PR
      if: github.event_name == 'pull_request' && hashFiles('pr_comment.md') != ''
      uses: actions/github-script@v7
      
      
      with:
        script: |
          const fs = require('fs');
          
          try {
            // Priority: Use regression comment if available, fallback to main comment
            let comment = '';
            
            if (fs.existsSync('pr_regression_comment.md')) {
              const regressionComment = fs.readFileSync('pr_regression_comment.md', 'utf8');
              
              if (fs.existsSync('pr_comment.md')) {
                const mainComment = fs.readFileSync('pr_comment.md', 'utf8');
                comment = regressionComment + '

---

' + mainComment;
              } else {
                comment = regressionComment;
              }
            } else if (fs.existsSync('pr_comment.md')) {
              comment = fs.readFileSync('pr_comment.md', 'utf8');
            }
            
            if (!comment) {
              console.log('No comment content found');
              return;
            }
            
            // Add regression warning to top if needed
            const hasRegression = process.env.PERFORMANCE_REGRESSION === 'true';
            if (hasRegression) {
              comment = 'ðŸš¨ **PERFORMANCE REGRESSION DETECTED**

' + comment;
            }

    - name: ðŸ“¤ Upload enhanced artifacts
      uses: actions/upload-artifact@v4
      if: always()
                  with:
        name: pr-performance-check-${{ github.run_id }}
        path: |
          data/results/latest_*.json
          data/results/*_benchmark_*.json
          data/results/pr_regression_report.json
          docs/results/*_report.html
          pr_comment.md
          pr_regression_comment.md
        retention-days: 30

    - name: âœ… Performance check complete
      run: |
        echo "âœ… PR performance check completed"
        echo "ðŸ“Š Interactive report and detailed analysis available in artifacts"
        echo "ðŸ’¬ PR comment posted with performance analysis" 