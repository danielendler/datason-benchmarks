name: ðŸš€ PR Performance Check

on:
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'External PR number'
        required: false
        type: string
      commit_sha:
        description: 'External commit SHA'
        required: false
        type: string
      artifact_name:
        description: 'External wheel artifact name'
        required: false
        type: string
      datason_repo:
        description: 'External DataSON repo (owner/repo)'
        required: false
        type: string
      benchmark_type:
        description: 'Benchmark type'
        required: false
        default: 'pr_optimized'
        type: string

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: 1
  CI: true
  GITHUB_ACTIONS: true
  PYTHONPATH: .

jobs:
  quick-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: ðŸ’¾ Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-
          pr-deps-${{ runner.os }}-py3.11-
          benchmark-deps-${{ runner.os }}-py3.11-

    - name: ðŸ“¥ Download external DataSON wheel (if triggered externally)
      if: github.event.inputs.datason_repo != '' && github.event.inputs.artifact_name != ''
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.BENCHMARK_REPO_TOKEN }}
        script: |
          const fs = require('fs');
          
          // Parse external repository info
          const [owner, repo] = '${{ github.event.inputs.datason_repo }}'.split('/');
          const artifactName = '${{ github.event.inputs.artifact_name }}';
          const commitSha = '${{ github.event.inputs.commit_sha }}';
          
          console.log(`ðŸ” Searching for external artifact: ${artifactName}`);
          console.log(`ðŸ“¦ Repository: ${owner}/${repo}`);
          console.log(`ðŸ”— Commit: ${commitSha}`);
          
          // Get workflow runs for the commit
          const runsResponse = await github.rest.actions.listWorkflowRunsForRepo({
            owner: owner,
            repo: repo,
            head_sha: commitSha,
            status: 'completed',
            per_page: 20
          });
          
          // Find the artifact from the most recent successful run
          let artifactId = null;
          for (const run of runsResponse.data.workflow_runs) {
            if (run.conclusion === 'success') {
              try {
                const artifactsResponse = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: owner,
                  repo: repo,
                  run_id: run.id
                });
                
                const artifact = artifactsResponse.data.artifacts.find(a => a.name === artifactName);
                if (artifact && !artifact.expired) {
                  console.log(`âœ… Found external artifact: ${artifact.name}`);
                  artifactId = artifact.id;
                  break;
                }
              } catch (error) {
                console.log(`âš ï¸ Could not access artifacts for run ${run.id}`);
              }
            }
          }
          
          if (!artifactId) {
            console.log(`âš ï¸ Could not find external artifact '${artifactName}' - using PyPI version`);
            return;
          }
          
          // Download the external artifact
          console.log('ðŸ“¥ Downloading external DataSON wheel...');
          const download = await github.rest.actions.downloadArtifact({
            owner: owner,
            repo: repo,
            artifact_id: artifactId,
            archive_format: 'zip'
          });
          
          // Save the artifact
          fs.mkdirSync('external-wheel', { recursive: true });
          fs.writeFileSync('external-wheel/datason-wheel.zip', Buffer.from(download.data));
          console.log('âœ… External DataSON wheel downloaded');

    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        
        # Install external DataSON wheel if available
        if [ -f external-wheel/datason-wheel.zip ]; then
          echo "ðŸ”§ Installing external DataSON wheel..."
          cd external-wheel
          unzip -q datason-wheel.zip
          WHEEL_FILE=$(find . -name "*.whl" -type f | head -n1)
          if [ -n "$WHEEL_FILE" ]; then
            echo "ðŸ“¦ Installing: $WHEEL_FILE"
            pip install "$WHEEL_FILE"
            python -c "import datason; print(f'âœ… External DataSON {datason.__version__} installed')"
          else
            echo "âš ï¸ No wheel file found in external artifact"
            pip install -r requirements.txt
          fi
          cd ..
        else
          echo "ðŸ“¦ Installing DataSON from requirements.txt"
          pip install -r requirements.txt
        fi
        
        # Install remaining dependencies
        pip install orjson ujson msgpack jsonpickle numpy pandas matplotlib plotly requests
        
        # Verify all competitive libraries are available
        python -c "
        import sys
        try:
            import datason, orjson, ujson, json, pickle, jsonpickle, msgpack
            print('âœ… All competitive libraries installed successfully')
            print(f'ðŸ“Š DataSON version: {datason.__version__}')
        except ImportError as e:
            print(f'âŒ Missing library: {e}')
            sys.exit(1)
        "

    - name: ðŸš€ Run PR benchmark suite
      run: |
        echo "ðŸš€ Running PR performance check..."
        
        # Ensure results directory exists
        mkdir -p data/results docs/results
        
        # Set Python path for proper module imports
        export PYTHONPATH="$PWD:$PYTHONPATH"
        
        # Run quick benchmark with report generation
        python scripts/run_benchmarks.py --quick --generate-report

        # Run Rust-core micro-benchmark
        DATASON_RUST=1 python scripts/bench_rust_core.py save_string --sizes 10k --shapes flat --repeat 1 --output rust_core_result.json

    - name: ðŸ“¦ Upload Rust-core results
      uses: actions/upload-artifact@v4
      with:
        name: rust-core-results
        path: rust_core_result.json
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}

    - name: ðŸ“Š Generate PR comment
      if: github.event_name == 'pull_request' || github.event.inputs.pr_number != ''
      run: |
        # Determine PR number and commit SHA for both internal and external triggers
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          PR_NUMBER="${{ github.event.number }}"
          COMMIT_SHA="${{ github.sha }}"
          BENCHMARK_TYPE="quick"
        else
          PR_NUMBER="${{ github.event.inputs.pr_number }}"
          COMMIT_SHA="${{ github.event.inputs.commit_sha }}"
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'pr_optimized' }}"
        fi
        
        if [ -f data/results/latest_quick.json ]; then
          # Select appropriate baseline based on benchmark type
          BASELINE_FILE=""
          if [ -f data/results/latest_competitive.json ] && [ ! -L data/results/latest_competitive.json -o -e data/results/latest_competitive.json ]; then
            BASELINE_FILE="data/results/latest_competitive.json"
          else
            # Find most recent competitive baseline file by modification time
            RECENT_COMPETITIVE=$(find data/results -name "*competitive*.json" -not -name "latest_competitive.json" -printf '%T@ %p\n' 2>/dev/null | sort -nr | head -1 | cut -d' ' -f2- || find data/results -name "*competitive*.json" -not -name "latest_competitive.json" -exec ls -t {} + 2>/dev/null | head -1)
            if [ -n "$RECENT_COMPETITIVE" ]; then
              BASELINE_FILE="$RECENT_COMPETITIVE"
              echo "ðŸ“Š Using recent competitive baseline: $(basename $BASELINE_FILE)"
            elif [ -f data/results/latest_quick.json ]; then
              # Use previous quick benchmark as baseline if competitive not available
              BASELINE_FILE="data/results/latest_quick.json"
              echo "âš ï¸ Warning: Using quick baseline instead of competitive baseline"
            elif [ -f data/results/latest.json ]; then
              # Fallback to general baseline (different format - may not be ideal)
              BASELINE_FILE="data/results/latest.json" 
              echo "âš ï¸ Warning: Using tiered baseline - format mismatch possible"
            fi
          fi
          
          if [ -n "$BASELINE_FILE" ]; then
            python scripts/generate_pr_comment.py \
              --pr-number "$PR_NUMBER" \
              --commit-sha "$COMMIT_SHA" \
              --benchmark-type "$BENCHMARK_TYPE" \
              --result-file data/results/latest_quick.json \
              --baseline-file "$BASELINE_FILE" \
              --output pr_comment.md
          else
            python scripts/generate_pr_comment.py \
              --pr-number "$PR_NUMBER" \
              --commit-sha "$COMMIT_SHA" \
              --benchmark-type "$BENCHMARK_TYPE" \
              --result-file data/results/latest_quick.json \
              --output pr_comment.md
          fi

          cat pr_comment.md >> "$GITHUB_STEP_SUMMARY"
          echo "ðŸ“Š PR comment generated"
        else
          echo "âš ï¸ No benchmark results found" | tee -a "$GITHUB_STEP_SUMMARY"
          echo "No benchmark results found" > pr_comment.md
        fi
    - name: ðŸ” Advanced Regression Detection
      run: |
        # Install regression detection dependencies if needed
        pip install pandas matplotlib 2>/dev/null || true
        
        # Find actual benchmark result files
        RESULT_FILES=$(find data/results -name "latest_*.json" -not -name "latest.json" 2>/dev/null | head -5)
        
        if [ -n "$RESULT_FILES" ]; then
          echo "ðŸ“Š Running advanced regression detection..."
          echo "Found result files: $RESULT_FILES"
          
          # Use same smart baseline selection as PR comment generation
          REGRESSION_BASELINE=""
          if [ -f data/results/latest_competitive.json ] && [ ! -L data/results/latest_competitive.json -o -e data/results/latest_competitive.json ]; then
            REGRESSION_BASELINE="data/results/latest_competitive.json"
          else
            # Find most recent competitive baseline file by modification time
            RECENT_COMPETITIVE=$(find data/results -name "*competitive*.json" -not -name "latest_competitive.json" -printf '%T@ %p\n' 2>/dev/null | sort -nr | head -1 | cut -d' ' -f2- || find data/results -name "*competitive*.json" -not -name "latest_competitive.json" -exec ls -t {} + 2>/dev/null | head -1)
            if [ -n "$RECENT_COMPETITIVE" ]; then
              REGRESSION_BASELINE="$RECENT_COMPETITIVE"
              echo "ðŸ“Š Using recent competitive baseline for regression detection: $(basename $REGRESSION_BASELINE)"
            elif [ -f data/results/latest.json ]; then
              REGRESSION_BASELINE="data/results/latest.json"
              echo "âš ï¸ Warning: Using tiered baseline for regression detection - format mismatch possible"
            fi
          fi
          
          if [ -n "$REGRESSION_BASELINE" ]; then
            # Run regression detection with correct baseline
            python scripts/regression_detector.py \
              data/results/latest_quick.json \
              --baseline "$REGRESSION_BASELINE" \
              --output data/results/pr_regression_report.json \
              --pr-comment pr_regression_comment.md \
              --fail-threshold 0.25 \
              --warn-threshold 0.10
          else
            echo "âš ï¸ No suitable baseline found for regression detection"
            echo "ðŸ”„ **No Baseline Available**" > pr_regression_comment.md
            echo "No suitable baseline found for performance comparison." >> pr_regression_comment.md
          fi
          
          REGRESSION_EXIT_CODE=$?
          echo "REGRESSION_EXIT_CODE=$REGRESSION_EXIT_CODE" >> $GITHUB_ENV
          
          if [ $REGRESSION_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Critical performance regressions detected!"
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          else
            echo "âœ… No critical regressions detected"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi
          
        else
          echo "ðŸ“ No baseline or benchmark results found - creating baseline comment"
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "ðŸ”„ **Performance Baseline**" > pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "This is the first benchmark run or no previous baseline was found." >> pr_regression_comment.md
          echo "Future PRs will be compared against this baseline." >> pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "**Available Results:**" >> pr_regression_comment.md
          if [ -n "$RESULT_FILES" ]; then
            for file in $RESULT_FILES; do
              echo "- $(basename $file)" >> pr_regression_comment.md
            done
          else
            echo "- No benchmark result files found" >> pr_regression_comment.md
          fi
        fi



    - name: ðŸ’¬ Update PR comment
      if: always() && (github.event_name == 'pull_request' || github.event.inputs.pr_number != '') && hashFiles('pr_comment.md') != ''
      run: |
        # Determine PR number for both internal and external triggers
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          PR_NUMBER="${{ github.event.number }}"
          TARGET_REPO="${{ github.repository }}"
        else
          PR_NUMBER="${{ github.event.inputs.pr_number }}"
          TARGET_REPO="${{ github.event.inputs.datason_repo }}"
        fi
        
        # Clean slate: remove any previous comment files to prevent old content
        echo "ðŸ§¹ Cleaning up previous comment files..."
        rm -f pr_regression_comment.md pr_comment.md final_pr_comment.md
        
        # Generate comprehensive comment by merging all available content
        echo "ðŸ“ Generating final PR comment..."
        
        # Create comprehensive performance comment
        cat > final_pr_comment.md << 'EOF'
        <!-- datason-benchmark-bot -->
        **DataSON PR Performance Analysis**
        EOF
        
        # Add PR and commit info
        if [ -n "$PR_NUMBER" ]; then
          echo "PR #$PR_NUMBER | Commit: ${{ github.event.inputs.commit_sha || github.sha }}" >> final_pr_comment.md
          echo "" >> final_pr_comment.md
        fi
        
        # Add regression analysis first (most important)
        if [ -f pr_regression_comment.md ]; then
          echo "ðŸ“Š Including regression analysis..."
          cat pr_regression_comment.md >> final_pr_comment.md
          echo "" >> final_pr_comment.md
          echo "---" >> final_pr_comment.md
          echo "" >> final_pr_comment.md
        else
          echo "â„¹ï¸ No regression analysis file found - skipping regression section"
        fi
        
        # Add main benchmark results
        if [ -f pr_comment.md ]; then
          echo "ðŸ“ˆ Including main benchmark results..."
          cat pr_comment.md >> final_pr_comment.md
          echo "" >> final_pr_comment.md
        else
          echo "â„¹ï¸ No main benchmark results file found - skipping results section"
        fi
        
        # Add footer
        cat >> final_pr_comment.md << 'EOF'
        
        ---
        *Generated by datason-benchmarks â€¢ Comprehensive Performance Analysis*
        EOF
        
        # Debug: Show what content we're about to post
        echo "ðŸ“ Final comment content preview:"
        echo "--- START COMMENT ---"
        head -20 final_pr_comment.md
        echo "... (truncated)"
        echo "--- END COMMENT ---"
        echo "ðŸ’¬ Comment file size: $(wc -c < final_pr_comment.md) characters"
        
        # Use the advanced comment manager to replace old comments cleanly
        if [ -n "$PR_NUMBER" ] && [ -n "$TARGET_REPO" ]; then
          echo "ðŸ”„ Updating PR #$PR_NUMBER comment in $TARGET_REPO using 'update' strategy..."
          python scripts/manage_pr_comments.py \
            --token "${{ secrets.BENCHMARK_REPO_TOKEN || secrets.GITHUB_TOKEN }}" \
            --repo "$TARGET_REPO" \
            --pr-number "$PR_NUMBER" \
            --comment-file final_pr_comment.md \
            --strategy "update"
        else
          echo "âš ï¸ Could not determine PR number or target repo - skipping comment update"
          echo "PR_NUMBER: $PR_NUMBER"
          echo "TARGET_REPO: $TARGET_REPO"
        fi

    - name: ðŸ“¤ Upload enhanced artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
          name: pr-performance-check-${{ github.run_id }}
          path: |
            data/results/latest_*.json
            data/results/*_benchmark_*.json
            data/results/pr_regression_report.json
            docs/results/*_report.html
            pr_comment.md
            pr_regression_comment.md
          retention-days: 30

    - name: âœ… Performance check complete
      run: |
        echo "âœ… PR performance check completed"
        echo "ðŸ“Š Interactive report and detailed analysis available in artifacts"
        echo "ðŸ’¬ PR comment posted with performance analysis" 
