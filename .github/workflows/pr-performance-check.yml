name: ðŸš€ PR Performance Check

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'
      - 'benchmarks/**'
      - 'competitors/**'
      - 'requirements.txt'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: 1
  CI: true
  GITHUB_ACTIONS: true

jobs:
  quick-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: ðŸ’¾ Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-${{ github.run_id }}
        restore-keys: |
          pr-deps-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}-
          pr-deps-${{ runner.os }}-py3.11-
          benchmark-deps-${{ runner.os }}-py3.11-

    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
        # Verify all competitive libraries are available
        python -c "
        import sys
        try:
            import datason, orjson, ujson, json, pickle, jsonpickle, msgpack
            print('âœ… All competitive libraries installed successfully')
        except ImportError as e:
            print(f'âŒ Missing library: {e}')
            sys.exit(1)
        "

    - name: ðŸš€ Run PR benchmark suite
      run: |
        echo "ðŸš€ Running PR performance check..."
        
        # Ensure results directory exists
        mkdir -p data/results docs/results
        
        # Run quick benchmark with report generation
        python scripts/run_benchmarks.py --quick --generate-report

        # Run Rust-core micro-benchmark
        DATASON_RUST=1 python scripts/bench_rust_core.py save_string --sizes 10k --shapes flat --repeat 1 --output rust_core_result.json

    - name: ðŸ“¦ Upload Rust-core results
      uses: actions/upload-artifact@v4
      with:
        name: rust-core-results
        path: rust_core_result.json
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}

    - name: ðŸ“Š Generate PR comment
      if: github.event_name == 'pull_request'
      run: |
        if [ -f data/results/latest_quick.json ]; then
          python scripts/generate_pr_comment.py \
            --pr-number "${{ github.event.number }}" \
            --commit-sha "${{ github.sha }}" \
            --benchmark-type "quick" \
            --result-file data/results/latest_quick.json \
            --baseline-file data/results/latest.json \
            --output pr_comment.md

          cat pr_comment.md >> "$GITHUB_STEP_SUMMARY"
          echo "ðŸ“Š PR comment generated"
        else
          echo "âš ï¸ No benchmark results found" | tee -a "$GITHUB_STEP_SUMMARY"
          echo "No benchmark results found" > pr_comment.md
        fi
    - name: ðŸ” Advanced Regression Detection
      run: |
        # Install regression detection dependencies if needed
        pip install pandas matplotlib 2>/dev/null || true
        
        # Run comprehensive regression detection
        if [ -f data/results/latest.json ]; then
          echo "ðŸ“Š Running advanced regression detection..."
          python scripts/regression_detector.py \
            data/results/latest_competitive_*.json \
            --baseline data/results/latest.json \
            --output data/results/pr_regression_report.json \
            --pr-comment pr_regression_comment.md \
            --fail-threshold 0.25 \
            --warn-threshold 0.10
          
          REGRESSION_EXIT_CODE=$?
          echo "REGRESSION_EXIT_CODE=$REGRESSION_EXIT_CODE" >> $GITHUB_ENV
          
          if [ $REGRESSION_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Critical performance regressions detected!"
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          else
            echo "âœ… No critical regressions detected"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi
          
        else
          echo "ðŸ“ No baseline found - this will become the new baseline"
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "ðŸ”„ **Performance Baseline**" > pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "This is the first benchmark run or no previous results were found." >> pr_regression_comment.md
          echo "Future PRs will be compared against this baseline." >> pr_regression_comment.md
        fi
    - name: ðŸ” Advanced Regression Detection
      run: |
        # Install regression detection dependencies if needed
        pip install pandas matplotlib 2>/dev/null || true
        
        # Run comprehensive regression detection
        if [ -f data/results/latest.json ]; then
          echo "ðŸ“Š Running advanced regression detection..."
          python scripts/regression_detector.py \
            data/results/latest_competitive_*.json \
            --baseline data/results/latest.json \
            --output data/results/pr_regression_report.json \
            --pr-comment pr_regression_comment.md \
            --fail-threshold 0.25 \
            --warn-threshold 0.10
          
          REGRESSION_EXIT_CODE=$?
          echo "REGRESSION_EXIT_CODE=$REGRESSION_EXIT_CODE" >> $GITHUB_ENV
          
          if [ $REGRESSION_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸ Critical performance regressions detected!"
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          else
            echo "âœ… No critical regressions detected"
            echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          fi
          
        else
          echo "ðŸ“ No baseline found - this will become the new baseline"
          echo "PERFORMANCE_REGRESSION=false" >> $GITHUB_ENV
          echo "ðŸ”„ **Performance Baseline**" > pr_regression_comment.md
          echo "" >> pr_regression_comment.md
          echo "This is the first benchmark run or no previous results were found." >> pr_regression_comment.md
          echo "Future PRs will be compared against this baseline." >> pr_regression_comment.md
        fi



    - name: ðŸ’¬ Comment on PR
      if: github.event_name == 'pull_request' && hashFiles('pr_comment.md') != ''
      uses: actions/github-script@v7
      continue-on-error: true
      with:
        script: |
          const fs = require('fs');

          try {
            // Priority: Use regression comment if available, fallback to main comment
            let comment = '';

            if (fs.existsSync('pr_regression_comment.md')) {
              const regressionComment = fs.readFileSync('pr_regression_comment.md', 'utf8');

              if (fs.existsSync('pr_comment.md')) {
                const mainComment = fs.readFileSync('pr_comment.md', 'utf8');
                comment = regressionComment + '\n---\n' + mainComment;
              } else {
                comment = regressionComment;
              }
            } else if (fs.existsSync('pr_comment.md')) {
              comment = fs.readFileSync('pr_comment.md', 'utf8');
            }

            if (!comment) {
              console.log('No comment content found');
              return;
            }

            // Add regression warning to top if needed
            const hasRegression = process.env.PERFORMANCE_REGRESSION === 'true';
            if (hasRegression) {
              comment = '\uD83D\uDEA8 **PERFORMANCE REGRESSION DETECTED**\n\n' + comment;
            }

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment,
            });
          } catch (error) {
            console.log(`Failed to post comment: ${error.message}`);
          }

    - name: ðŸ“¤ Upload enhanced artifacts
      uses: actions/upload-artifact@v4
      if: always()
        with:
          name: pr-performance-check-${{ github.run_id }}
          path: |
            data/results/latest_*.json
            data/results/*_benchmark_*.json
            data/results/pr_regression_report.json
            docs/results/*_report.html
            pr_comment.md
            pr_regression_comment.md
          retention-days: 30

    - name: âœ… Performance check complete
      run: |
        echo "âœ… PR performance check completed"
        echo "ðŸ“Š Interactive report and detailed analysis available in artifacts"
        echo "ðŸ’¬ PR comment posted with performance analysis" 
