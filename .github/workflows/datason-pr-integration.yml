name: ðŸ§ª DataSON PR Benchmark

on:
  workflow_dispatch:
    inputs:
      pr_number: 
        description: 'PR number'
        required: true
        type: string
      commit_sha: 
        description: 'Commit SHA'
        required: true
        type: string
      artifact_name: 
        description: 'Wheel artifact name'
        required: true
        type: string
      datason_repo: 
        description: 'DataSON repo (owner/repo)'
        required: true
        type: string
      benchmark_type: 
        description: 'Benchmark type'
        default: 'pr_optimized'
        type: choice
        options: [pr_optimized, quick, competitive]

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: ðŸ“¥ Checkout benchmarks
      uses: actions/checkout@v4

    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with: 
        python-version: "3.11"

    - name: ðŸ’¾ Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: datason-pr-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}

    - name: ðŸ“¦ Install dependencies
      run: |
        pip install -r requirements.txt
        pip install orjson ujson msgpack pandas numpy

    - name: ðŸ“¥ Download DataSON wheel from external repository
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.BENCHMARK_REPO_TOKEN }}
        script: |
          const fs = require('fs');

          // Parse repository info
          const [owner, repo] = '${{ github.event.inputs.datason_repo }}'.split('/');
          const artifactName = '${{ github.event.inputs.artifact_name }}';
          const commitSha = '${{ github.event.inputs.commit_sha }}';

          console.log(`ðŸ” Searching for artifact: ${artifactName}`);
          console.log(`ðŸ“¦ Repository: ${owner}/${repo}`);
          console.log(`ðŸ”— Commit: ${commitSha}`);

          // Get workflow runs for the commit
          const runsResponse = await github.rest.actions.listWorkflowRunsForRepo({
            owner: owner,
            repo: repo,
            head_sha: commitSha,
            status: 'completed',
            per_page: 20
          });

          console.log(`Found ${runsResponse.data.workflow_runs.length} completed runs`);

          // Find the artifact from the most recent successful run
          let artifactId = null;
          for (const run of runsResponse.data.workflow_runs) {
            if (run.conclusion === 'success') {
              console.log(`ðŸ” Checking run ${run.id} (${run.name})`);

              try {
                const artifactsResponse = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: owner,
                  repo: repo,
                  run_id: run.id
                });

                const artifact = artifactsResponse.data.artifacts.find(a => a.name === artifactName);
                if (artifact && !artifact.expired) {
                  console.log(`âœ… Found artifact: ${artifact.name} (${artifact.size_in_bytes} bytes)`);
                  artifactId = artifact.id;
                  break;
                }
              } catch (error) {
                console.log(`âš ï¸ Could not access artifacts for run ${run.id}: ${error.message}`);
              }
            }
          }

          if (!artifactId) {
            throw new Error(`âŒ Could not find artifact '${artifactName}' for commit ${commitSha}`);
          }

          // Download the artifact
          console.log('ðŸ“¥ Downloading artifact...');
          const download = await github.rest.actions.downloadArtifact({
            owner: owner,
            repo: repo,
            artifact_id: artifactId,
            archive_format: 'zip'
          });

          // Save the artifact
          fs.mkdirSync('wheel', { recursive: true });
          fs.writeFileSync('wheel/artifact.zip', Buffer.from(download.data));

          console.log('âœ… Artifact downloaded successfully');

    - name: ðŸ”§ Extract and install DataSON wheel
      run: |
        cd wheel
        unzip -q artifact.zip
        ls -la
        echo "ðŸ“¦ Extracted files:"
        find . -name "*.whl" -type f

        # Install the wheel
        WHEEL_FILE=$(find . -name "*.whl" -type f | head -n1)
        if [ -z "$WHEEL_FILE" ]; then
          echo "âŒ No wheel file found in artifact"
          exit 1
        fi

        echo "ðŸ”§ Installing: $WHEEL_FILE"
        pip install "$WHEEL_FILE"

        # Verify installation
        python -c "import datason; print(f'âœ… DataSON {datason.__version__} installed successfully')"

    - name: ðŸš€ Run benchmarks
      run: |
        mkdir -p data/results docs/results
        
        # Run the appropriate benchmark based on type
        case "${{ github.event.inputs.benchmark_type }}" in
          "pr_optimized")
            echo "ðŸŽ¯ Running PR-optimized benchmark suite..."
            python scripts/pr_optimized_benchmark.py --output data/results/pr_${{ github.event.inputs.pr_number }}.json
            ;;
          "quick")
            echo "âš¡ Running quick benchmark suite..."
            python scripts/run_benchmarks.py --quick --output data/results/pr_${{ github.event.inputs.pr_number }}.json
            ;;
          "competitive")
            echo "ðŸ Running competitive benchmark suite..."
            python scripts/run_benchmarks.py --competitive --output data/results/pr_${{ github.event.inputs.pr_number }}.json
            ;;
        esac

    - name: ðŸŽ¨ Generate enhanced report
      run: |
        # Generate enhanced report if result file exists
        if [ -f data/results/pr_${{ github.event.inputs.pr_number }}.json ]; then
          echo "ðŸŽ¨ Generating enhanced report..."
          
          # Use standard report generation (reliable fallback)
          if [ -f scripts/generate_report.py ]; then
            echo "Generating standard HTML report..."
            python scripts/generate_report.py \
              --input-dir data/results/ \
              --output docs/results/pr_${{ github.event.inputs.pr_number }}_report.html \
              --format html \
              --include-charts || echo "Report generation failed, continuing..."
          fi
          
          # Optional: Try Phase 4 enhanced reports (if working)
          if [ -f scripts/phase4_enhanced_reports.py ]; then
            echo "Attempting Phase 4 enhanced reporting..."
            python scripts/phase4_enhanced_reports.py \
              data/results/pr_${{ github.event.inputs.pr_number }}.json \
              --output-dir docs/results/ 2>/dev/null || echo "Phase 4 reports not available, using standard reports"
          fi
        else
          echo "âš ï¸ No result file found at data/results/pr_${{ github.event.inputs.pr_number }}.json"
        fi

    - name: ðŸ” Regression detection
      run: |
        # Compare against baseline if available
        if [ -f data/results/datason_baseline.json ] && [ -f scripts/regression_detector.py ]; then
          python scripts/regression_detector.py \
            data/results/pr_${{ github.event.inputs.pr_number }}.json \
            --baseline data/results/datason_baseline.json \
            --fail-threshold 0.30 \
            --warn-threshold 0.15 \
            --output regression_analysis.md
          
          echo "REGRESSION_STATUS=$?" >> $GITHUB_ENV
        else
          echo "ðŸ“ No baseline available for regression detection"
          echo "REGRESSION_STATUS=0" >> $GITHUB_ENV
        fi

    - name: ðŸ’¬ Generate PR comment
      run: |
        # Generate informative PR comment with actual performance data
        if [ -f data/results/pr_${{ github.event.inputs.pr_number }}.json ]; then
          echo "ðŸ“ Generating informative PR comment with performance data..."
          
          # Use the enhanced comment generator
          BASELINE_ARG=""
          if [ -f data/results/datason_baseline.json ]; then
            BASELINE_ARG="--baseline-file data/results/datason_baseline.json"
          fi
          
          python scripts/generate_pr_comment.py \
            --pr-number "${{ github.event.inputs.pr_number }}" \
            --commit-sha "${{ github.event.inputs.commit_sha }}" \
            --benchmark-type "${{ github.event.inputs.benchmark_type }}" \
            --result-file "data/results/pr_${{ github.event.inputs.pr_number }}.json" \
            $BASELINE_ARG \
            --output comment.md || echo "âš ï¸ Enhanced comment generation failed, using fallback..."
        fi
        
        # Fallback to simple comment if enhanced generation failed
        if [ ! -f comment.md ]; then
          echo "ðŸ“ Using fallback comment generation..."
          cat > comment.md << 'EOF'
        # ðŸš€ DataSON PR Performance Analysis
        
        **PR #${{ github.event.inputs.pr_number }}** | **Commit**: `${{ github.event.inputs.commit_sha }}`
        
        ## ðŸ“Š Benchmark Results
        
        âœ… Benchmarks completed successfully using **${{ github.event.inputs.benchmark_type }}** suite
        
        ## âœ… Status: Ready for Review
        
        Performance analysis completed. Detailed results available in workflow artifacts.
        
        ---
        *Generated by [datason-benchmarks](https://github.com/danielendler/datason-benchmarks) â€¢ Phase 1-4 Testing Suite*
        EOF
        fi

    - name: ðŸ“ Post comment to DataSON PR
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.BENCHMARK_REPO_TOKEN }}
        script: |
          const fs = require('fs');
          const comment = fs.readFileSync('comment.md', 'utf8');
          const [owner, repo] = '${{ github.event.inputs.datason_repo }}'.split('/');

          await github.rest.issues.createComment({
            issue_number: ${{ github.event.inputs.pr_number }},
            owner, repo, body: comment
          });

    - name: ðŸ“¤ Upload results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.event.inputs.pr_number }}
        path: |
          data/results/pr_${{ github.event.inputs.pr_number }}.json
          docs/results/*.html
          comment.md
          regression_analysis.md
        retention-days: 30

    - name: âŒ Fail on significant regression
      if: env.REGRESSION_STATUS != '0'
      run: |
        echo "âŒ Significant performance regression detected!"
        echo "This workflow will fail to alert the PR author to review performance impact."
        exit 1 