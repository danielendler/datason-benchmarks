name: 🧪 DataSON PR Benchmark

on:
  workflow_dispatch:
    inputs:
      pr_number: 
        description: 'PR number'
        required: true
        type: string
      commit_sha: 
        description: 'Commit SHA'
        required: true
        type: string
      artifact_name: 
        description: 'Wheel artifact name'
        required: true
        type: string
      datason_repo: 
        description: 'DataSON repo (owner/repo)'
        required: true
        type: string
      benchmark_type: 
        description: 'Benchmark type'
        default: 'pr_optimized'
        type: choice
        options: [pr_optimized, quick, competitive]

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: 📥 Checkout benchmarks
      uses: actions/checkout@v4

    - name: 🐍 Setup Python
      uses: actions/setup-python@v5
      with: 
        python-version: "3.11"

    - name: 💾 Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: datason-pr-${{ runner.os }}-py3.11-${{ hashFiles('requirements.txt') }}

    - name: 📦 Install dependencies
      run: |
        pip install -r requirements.txt
        pip install orjson ujson msgpack pandas numpy

    - name: 📥 Download DataSON wheel from external repository
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.BENCHMARK_REPO_TOKEN }}
        script: |
          const fs = require('fs');

          // Parse repository info
          const [owner, repo] = '${{ github.event.inputs.datason_repo }}'.split('/');
          const artifactName = '${{ github.event.inputs.artifact_name }}';
          const commitSha = '${{ github.event.inputs.commit_sha }}';

          console.log(`🔍 Searching for artifact: ${artifactName}`);
          console.log(`📦 Repository: ${owner}/${repo}`);
          console.log(`🔗 Commit: ${commitSha}`);

          // Get workflow runs for the commit
          const runsResponse = await github.rest.actions.listWorkflowRunsForRepo({
            owner: owner,
            repo: repo,
            head_sha: commitSha,
            status: 'completed',
            per_page: 20
          });

          console.log(`Found ${runsResponse.data.workflow_runs.length} completed runs`);

          // Find the artifact from the most recent successful run
          let artifactId = null;
          for (const run of runsResponse.data.workflow_runs) {
            if (run.conclusion === 'success') {
              console.log(`🔍 Checking run ${run.id} (${run.name})`);

              try {
                const artifactsResponse = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: owner,
                  repo: repo,
                  run_id: run.id
                });

                const artifact = artifactsResponse.data.artifacts.find(a => a.name === artifactName);
                if (artifact && !artifact.expired) {
                  console.log(`✅ Found artifact: ${artifact.name} (${artifact.size_in_bytes} bytes)`);
                  artifactId = artifact.id;
                  break;
                }
              } catch (error) {
                console.log(`⚠️ Could not access artifacts for run ${run.id}: ${error.message}`);
              }
            }
          }

          if (!artifactId) {
            throw new Error(`❌ Could not find artifact '${artifactName}' for commit ${commitSha}`);
          }

          // Download the artifact
          console.log('📥 Downloading artifact...');
          const download = await github.rest.actions.downloadArtifact({
            owner: owner,
            repo: repo,
            artifact_id: artifactId,
            archive_format: 'zip'
          });

          // Save the artifact
          fs.mkdirSync('wheel', { recursive: true });
          fs.writeFileSync('wheel/artifact.zip', Buffer.from(download.data));

          console.log('✅ Artifact downloaded successfully');

    - name: 🔧 Extract and install DataSON wheel
      run: |
        cd wheel
        unzip -q artifact.zip
        ls -la
        echo "📦 Extracted files:"
        find . -name "*.whl" -type f

        # Install the wheel
        WHEEL_FILE=$(find . -name "*.whl" -type f | head -n1)
        if [ -z "$WHEEL_FILE" ]; then
          echo "❌ No wheel file found in artifact"
          exit 1
        fi

        echo "🔧 Installing: $WHEEL_FILE"
        pip install "$WHEEL_FILE"

        # Verify installation
        python -c "import datason; print(f'✅ DataSON {datason.__version__} installed successfully')"

    - name: 🚀 Run benchmarks and profiling
      run: |
        mkdir -p data/results docs/results
        
        # Run DataSON profiling demo to get detailed performance metrics
        echo "🔍 Running DataSON profiling analysis..."
        DATASON_PROFILE=1 python scripts/datason_profiling_demo.py | tee data/results/pr_${{ github.event.inputs.pr_number }}_profiling.txt
        
        # Also run profiling on baseline version if available
        if [ -f data/baseline/datason_baseline.whl ]; then
          echo "🔄 Running baseline profiling for comparison..."
          pip uninstall -y datason
          pip install data/baseline/datason_baseline.whl
          DATASON_PROFILE=1 python scripts/datason_profiling_demo.py | tee data/results/baseline_profiling.txt
          # Reinstall PR version
          pip uninstall -y datason
          pip install wheel/*.whl
        fi
        
        # Run the appropriate benchmark suite based on type
        case "${{ github.event.inputs.benchmark_type }}" in
          "pr_optimized")
            echo "🎯 Running PR-optimized benchmark suite..."
            python scripts/pr_optimized_benchmark.py --output data/results/pr_${{ github.event.inputs.pr_number }}.json
            ;;
          "quick")
            echo "⚡ Running quick benchmark suite..."
            python scripts/run_benchmarks.py --quick --output data/results/pr_${{ github.event.inputs.pr_number }}.json
            ;;
          "competitive")
            echo "🏁 Running competitive benchmark suite..."
            python scripts/run_benchmarks.py --competitive --output data/results/pr_${{ github.event.inputs.pr_number }}.json
            ;;
        esac

    - name: 🎨 Generate enhanced report
      run: |
        # Generate enhanced report if result file exists
        if [ -f data/results/pr_${{ github.event.inputs.pr_number }}.json ]; then
          echo "🎨 Generating enhanced report..."
          
          # Use standard report generation (reliable fallback)
          if [ -f scripts/generate_report.py ]; then
            echo "Generating standard HTML report..."
            python scripts/generate_report.py \
              --input-dir data/results/ \
              --output docs/results/pr_${{ github.event.inputs.pr_number }}_report.html \
              --format html \
              --include-charts || echo "Report generation failed, continuing..."
          fi
          
          # Optional: Try Phase 4 enhanced reports (if working)
          if [ -f scripts/phase4_enhanced_reports.py ]; then
            echo "Attempting Phase 4 enhanced reporting..."
            python scripts/phase4_enhanced_reports.py \
              data/results/pr_${{ github.event.inputs.pr_number }}.json \
              2>/dev/null || echo "Phase 4 reports not available, using standard reports"
          fi
        else
          echo "⚠️ No result file found at data/results/pr_${{ github.event.inputs.pr_number }}.json"
        fi

    - name: 🔍 Regression detection
      run: |
        # Compare against baseline if available
        if [ -f data/results/datason_baseline.json ] && [ -f scripts/regression_detector.py ]; then
          python scripts/regression_detector.py \
            data/results/pr_${{ github.event.inputs.pr_number }}.json \
            --baseline data/results/datason_baseline.json \
            --fail-threshold 0.30 \
            --warn-threshold 0.15 \
            --output regression_analysis.md
          
          echo "REGRESSION_STATUS=$?" >> $GITHUB_ENV
        else
          echo "📝 No baseline available for regression detection"
          echo "REGRESSION_STATUS=0" >> $GITHUB_ENV
        fi

    - name: 💬 Generate enhanced PR comment with real metrics
      run: |
        echo "📝 Generating PR comment with actual performance metrics..."
        
        # First try to generate comment from profiling data
        if [ -f "data/results/pr_${{ github.event.inputs.pr_number }}_profiling.txt" ]; then
          echo "Generating performance report from profiling data..."
          
          if [ -f "data/results/baseline_profiling.txt" ]; then
            # Generate comparison report
            python scripts/extract_performance_metrics.py \
              "data/results/pr_${{ github.event.inputs.pr_number }}_profiling.txt" \
              "data/results/baseline_profiling.txt" > comment.md
          else
            # Generate single report
            python scripts/extract_performance_metrics.py \
              "data/results/pr_${{ github.event.inputs.pr_number }}_profiling.txt" > comment.md
          fi
          
          # Append additional benchmark data if available
          if [ -f "data/results/pr_${{ github.event.inputs.pr_number }}.json" ]; then
            echo "" >> comment.md
            echo "## 📦 Additional Benchmark Data" >> comment.md
            echo "Full benchmark suite results are available in the workflow artifacts." >> comment.md
          fi
        
        # Fallback to standard comment generator if profiling extraction fails
        elif [ -f data/results/pr_${{ github.event.inputs.pr_number }}.json ]; then
          echo "Falling back to standard comment generation..."
          
          BASELINE_ARG=""
          if [ -f data/results/datason_baseline.json ]; then
            BASELINE_ARG="--baseline-file data/results/datason_baseline.json"
          fi
          
          python scripts/generate_pr_comment.py \
            --pr-number "${{ github.event.inputs.pr_number }}" \
            --commit-sha "${{ github.event.inputs.commit_sha }}" \
            --benchmark-type "${{ github.event.inputs.benchmark_type }}" \
            --result-file "data/results/pr_${{ github.event.inputs.pr_number }}.json" \
            $BASELINE_ARG \
            --output comment.md || echo "⚠️ Comment generation failed"
        fi
        
        # Final fallback to simple comment
        if [ ! -f comment.md ]; then
          echo "📝 Using fallback comment..."
          cat > comment.md << 'EOF'
        # 🚀 DataSON PR Performance Analysis
        
        **PR #${{ github.event.inputs.pr_number }}** | **Commit**: `${{ github.event.inputs.commit_sha }}`
        
        ## 📊 Benchmark Results
        
        ✅ Benchmarks completed successfully using **${{ github.event.inputs.benchmark_type }}** suite
        
        ## ✅ Status: Ready for Review
        
        Performance analysis completed. Detailed results available in workflow artifacts.
        
        ---
        *Generated by [datason-benchmarks](https://github.com/danielendler/datason-benchmarks)*
        EOF
        fi

    - name: 📝 Post or update comment on DataSON PR  
      run: |
        echo "🔄 Managing PR comments to avoid duplication..."
        python scripts/manage_pr_comments.py \
          --token "${{ secrets.BENCHMARK_REPO_TOKEN }}" \
          --repo "${{ github.event.inputs.datason_repo }}" \
          --pr-number ${{ github.event.inputs.pr_number }} \
          --comment-file comment.md \
          --strategy update

    - name: 📤 Upload results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.event.inputs.pr_number }}
        path: |
          data/results/pr_${{ github.event.inputs.pr_number }}.json
          data/results/pr_${{ github.event.inputs.pr_number }}_profiling.txt
          data/results/baseline_profiling.txt
          docs/results/*.html
          comment.md
          regression_analysis.md
          performance_metrics.json
        retention-days: 30

    - name: ❌ Fail on significant regression
      if: env.REGRESSION_STATUS != '0'
      run: |
        echo "❌ Significant performance regression detected!"
        echo "This workflow will fail to alert the PR author to review performance impact."
        exit 1 