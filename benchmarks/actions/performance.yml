name: üìä Performance Tracking (Informational)

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'datason/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
  pull_request:
    branches: [ main ]
    paths:
      - 'datason/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
  schedule:
    # Run weekly to track performance over time
    - cron: '0 8 * * 1'  # Every Monday at 8 AM UTC
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save current results as new baseline'
        required: false
        default: false
        type: boolean
      regression_threshold:
        description: 'Regression threshold percentage (default: 25)'
        required: false
        default: '25'
        type: string

permissions:
  contents: read
  pages: write
  id-token: write

env:
  PYTHONUNBUFFERED: 1
  # Environment-aware threshold - CI environments are more variable
  PERFORMANCE_REGRESSION_THRESHOLD: ${{ github.event.inputs.regression_threshold || '25' }}

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis

    - name: üêç Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: üíæ Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: performance-${{ runner.os }}-py3.11-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          performance-${{ runner.os }}-py3.11-

    - name: üì¶ Install package and dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest  # Minimal dependencies for performance testing

    - name: üìÇ Create results directory
      run: mkdir -p benchmarks/results

    - name: üì• Download previous CI baseline (if available)
      uses: actions/cache@v4
      with:
        path: benchmarks/results
        key: performance-baseline-ci-${{ runner.os }}-${{ github.repository }}
        restore-keys: |
          performance-baseline-ci-${{ runner.os }}-

    - name: üöÄ Run performance benchmarks
      run: |
        cd benchmarks
        # Set environment-aware threshold
        export PERFORMANCE_REGRESSION_THRESHOLD="${{ env.PERFORMANCE_REGRESSION_THRESHOLD }}"
        python ci_performance_tracker.py
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}
        RUNNER_OS: ${{ runner.os }}
        CI_ENVIRONMENT: true

    - name: üìã Save baseline (if requested or weekly)
      if: ${{ github.event.inputs.save_baseline == 'true' || (github.ref == 'refs/heads/main' && github.event_name == 'schedule') }}
      run: |
        cd benchmarks
        cp results/latest.json results/baseline_ci.json
        echo "‚úÖ Saved new CI baseline"

    - name: üìä Generate performance report
      run: |
        cd benchmarks
        python -c "
        import json
        import os

        # Load latest comparison
        if os.path.exists('results/latest_comparison.json'):
            with open('results/latest_comparison.json', 'r') as f:
                comparison = json.load(f)

            # Create GitHub Actions summary
            with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
                f.write('# üìä Performance Test Results (Informational)\n\n')
                f.write('> **Note**: Performance tests are informational and do not block CI due to environment variability.\n\n')

                if comparison['status'] == 'baseline_created':
                    f.write('‚úÖ **Baseline created successfully**\n\n')
                    f.write('This is the first CI run, so results have been saved as the new CI baseline.\n')
                else:
                    f.write(f'üìã **Compared with baseline from:** {comparison[\"baseline_metadata\"].get(\"timestamp\", \"unknown\")}\n\n')

                    # Note about environment differences
                    if comparison.get('environment_mismatch'):
                        f.write('‚ö†Ô∏è **Environment Difference Detected**: Baseline from different environment, results may not be directly comparable.\n\n')

                    if comparison['regressions']:
                        f.write(f'## üî¥ Performance Regressions ({len(comparison[\"regressions\"])}) - Threshold: {os.environ.get(\"PERFORMANCE_REGRESSION_THRESHOLD\", \"25\")}%\n\n')
                        f.write('| Test | Change | Current | Baseline | Severity |\n')
                        f.write('|------|--------|---------|----------|----------|\n')
                        for reg in comparison['regressions']:
                            severity = 'üî• Major' if reg[\"change_pct\"] > 50 else '‚ö†Ô∏è Minor'
                            f.write(f'| {reg[\"test\"]} | {reg[\"change_pct\"]:+.1f}% | {reg[\"current_ms\"]:.2f}ms | {reg[\"baseline_ms\"]:.2f}ms | {severity} |\n')
                        f.write('\n')

                    if comparison['improvements']:
                        f.write(f'## üü¢ Performance Improvements ({len(comparison[\"improvements\"])})\n\n')
                        f.write('| Test | Change | Current | Baseline |\n')
                        f.write('|------|--------|---------|----------|\n')
                        for imp in comparison['improvements']:
                            f.write(f'| {imp[\"test\"]} | {imp[\"change_pct\"]:+.1f}% | {imp[\"current_ms\"]:.2f}ms | {imp[\"baseline_ms\"]:.2f}ms |\n')
                        f.write('\n')

                    if not comparison['regressions'] and not comparison['improvements']:
                        f.write('üü° **No significant performance changes detected**\n\n')
                        f.write(f'All performance metrics are within {os.environ.get(\"PERFORMANCE_REGRESSION_THRESHOLD\", \"25\")}% of baseline values.\n')

                    # Add guidance for interpreting results
                    f.write('## üìù Interpreting Results\n\n')
                    f.write('- **Environment Impact**: CI runners have variable performance; differences <50% may be environment noise\n')
                    f.write('- **Focus on Patterns**: Look for consistent regressions across multiple tests\n')
                    f.write('- **Major Regressions**: Changes >100% warrant investigation\n')
                    f.write('- **Micro-benchmarks**: Tests <1ms are especially susceptible to environment variation\n')
        "

    - name: üíæ Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results-${{ github.run_id }}
        path: |
          benchmarks/results/*.json
        retention-days: 90

    - name: üíæ Save CI baseline cache
      uses: actions/cache/save@v4
      if: github.ref == 'refs/heads/main'
      with:
        path: benchmarks/results
        key: performance-baseline-ci-${{ runner.os }}-${{ github.repository }}-${{ github.run_id }}

    - name: ‚ÑπÔ∏è Performance Status (Informational Only)
      if: always()
      run: |
        cd benchmarks
        echo "üìä Performance tests completed successfully"
        echo "‚úÖ Results are informational and do not block CI"

        if [ -f "results/latest_comparison.json" ]; then
          python -c "
          import json

          with open('results/latest_comparison.json', 'r') as f:
              comparison = json.load(f)

          if comparison.get('regressions'):
              major_regressions = [r for r in comparison['regressions'] if r['change_pct'] > 100]
              if major_regressions:
                  print(f'‚ö†Ô∏è Notice: {len(major_regressions)} major performance regressions detected (>100%)')
                  print('   Consider investigating if this was expected')
              else:
                  print(f'‚ÑπÔ∏è Info: {len(comparison[\"regressions\"])} minor performance regressions detected')
                  print('   Likely due to environment differences')
          else:
              print('‚úÖ No significant performance regressions detected')
          "
        fi

        # Always exit 0 - performance tests are informational
        exit 0

  # Optional: Generate historical performance charts
  generate-charts:
    runs-on: ubuntu-latest
    needs: performance-test
    if: github.ref == 'refs/heads/main'

    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4

    - name: üêç Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: üì¶ Install chart dependencies
      run: |
        pip install matplotlib seaborn pandas

    - name: üì• Download performance results
      uses: actions/download-artifact@v4
      with:
        name: performance-results-${{ github.run_id }}
        path: benchmarks/results

    - name: üìä Generate performance charts
      run: |
        cd benchmarks
        python -c "
        import json
        import matplotlib.pyplot as plt
        import pandas as pd
        from datetime import datetime
        import os
        import glob
        import re

        # Collect all historical results
        result_files = glob.glob('results/performance_comprehensive_*.json')
        result_files.extend(glob.glob('results/comprehensive_performance_*.json'))  # Also include comprehensive workflow files

        if len(result_files) < 2:
            print('Not enough historical data for charts')
            exit(0)

        data = []
        for file in sorted(result_files):
            with open(file, 'r') as f:
                result = json.load(f)

                # Get version info
                version = result.get('metadata', {}).get('datason_version', 'unknown')
                timestamp = result['metadata']['timestamp']

                # Skip experimental data (same day with many commits)
                # Only include data from different days or explicit version tags
                file_date = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).date()

                # Extract key ML benchmark metrics with shorter names
                ml_benchmarks = result.get('ml_benchmarks', {})
                for category, tests in ml_benchmarks.items():
                    for test_name, test_data in tests.items():
                        # Extract datason standard performance only
                        if 'datason_standard' in test_data:
                            # Create shorter, more readable test names
                            short_name = f\"{category.replace('_', ' ').title()}: {test_name.replace('_', ' ').title()}\"
                            data.append({
                                'version': version,
                                'timestamp': timestamp,
                                'date': file_date,
                                'test': short_name,
                                'time_ms': test_data['datason_standard']['mean'] * 1000
                            })

        if not data:
            print('No performance data found')
            exit(0)

        df = pd.DataFrame(data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        print(f'Found {len(df)} data points from {len(df["timestamp"].dt.date.unique())} day(s)')
        print(f'Versions: {sorted(df["version"].unique())}')
        print(f'Date range: {df["timestamp"].min()} to {df["timestamp"].max()}')

        # Group by date and version to reduce noise from experimental runs
        daily_data = []
        for (date, version), group in df.groupby(['date', 'version']):
            # Take the mean of all tests from the same day/version
            for test in group['test'].unique():
                test_data = group[group['test'] == test]
                if len(test_data) > 0:
                    daily_data.append({
                        'version': version,
                        'date': date,
                        'test': test,
                        'time_ms': test_data['time_ms'].mean()  # Average if multiple runs
                    })

        if not daily_data:
            print('No aggregated performance data found')
            exit(0)

        df_clean = pd.DataFrame(daily_data)
        print(f'After aggregation: {len(df_clean)} data points')

        # Check if we have enough version-based data for meaningful trends
        unique_dates = df_clean['date'].nunique()
        unique_versions = df_clean['version'].nunique()

        if unique_dates == 1 and unique_versions == 1:
            print('üß™ Single-day experimental data detected, using time-based experimental chart')

            # Use experimental approach with commit info if available
            fig, ax = plt.subplots(figsize=(16, 10))

            # Get git commit info for experimental data
            for file in sorted(result_files):
                with open(file, 'r') as f:
                    result = json.load(f)
                    git_info = result.get('version_info', {}).get('git_info', {})
                    if git_info.get('commit'):
                        commit_short = git_info['commit'][:8]
                        # Update data with commit info for debugging
                        break

            # Plot experimental trends over time
            top_tests = df['test'].value_counts().head(3).index
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
            markers = ['o', 's', '^']

            for i, test in enumerate(top_tests):
                test_data = df[df['test'] == test].sort_values('timestamp')
                ax.plot(test_data['timestamp'], test_data['time_ms'],
                       marker=markers[i], color=colors[i], linewidth=2,
                       markersize=8, label=test, alpha=0.8)

            ax.set_title('Datason Experimental Performance (Development Session)', fontsize=16, fontweight='bold')
            ax.set_xlabel('Time', fontsize=12)
            ax.set_ylabel('Time (milliseconds)', fontsize=12)
            ax.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)

        else:
            print(f'üìà Multi-version data detected ({unique_versions} versions, {unique_dates} dates), using version-based chart')

            # Create a more readable chart
            fig, ax = plt.subplots(figsize=(14, 8))

            # Use only top 3 most representative tests for clarity
            top_tests = df_clean['test'].value_counts().head(3).index
            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green
            markers = ['o', 's', '^']  # Circle, Square, Triangle

            for i, test in enumerate(top_tests):
                test_data = df_clean[df_clean['test'] == test].sort_values('date')
                if len(test_data) > 1:  # Only plot if we have multiple data points
                    ax.plot(test_data['date'], test_data['time_ms'],
                           marker=markers[i], color=colors[i], linewidth=2,
                           markersize=8, label=test, alpha=0.8)

                    # Add version labels at key points
                    for _, row in test_data.iterrows():
                        if row['version'] not in ['unknown', '0.4.5']:  # Skip dev versions
                            ax.annotate(f'v{row["version"]}',
                                      (row['date'], row['time_ms']),
                                      xytext=(0, 10), textcoords='offset points',
                                      fontsize=8, alpha=0.7, ha='center')

            ax.set_title('Datason Performance Trends by Version', fontsize=16, fontweight='bold')
            ax.set_xlabel('Date', fontsize=12)
            ax.set_ylabel('Time (milliseconds)', fontsize=12)

            # Improve legend
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left',
                     frameon=True, fancybox=True, shadow=True)

            # Format dates on x-axis
            fig.autofmt_xdate()

        # Add common formatting for both chart types
        ax.grid(True, alpha=0.3, linestyle='--')
        plt.tight_layout()
        plt.savefig('performance_chart.png', dpi=150, bbox_inches='tight',
                   facecolor='white', edgecolor='none')
        print('‚úÖ Performance chart generated with adaptive formatting')
        "

    - name: üìä Upload performance chart
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-chart-${{ github.run_id }}
        path: benchmarks/performance_chart.png
        retention-days: 30
