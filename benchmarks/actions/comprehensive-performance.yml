name: ðŸ”¬ Comprehensive Performance Analysis

on:
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - with-ml
          - full-competitive
      save_results:
        description: 'Save results as benchmark reference'
        required: false
        default: false
        type: boolean
  schedule:
    # Run monthly comprehensive analysis
    - cron: '0 2 1 * *'  # First day of month at 2 AM UTC
  push:
    branches: [ main ]
    paths:
      - 'benchmarks/comprehensive_performance_suite.py'

permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  PYTHONUNBUFFERED: 1

jobs:
  comprehensive-performance:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      matrix:
        python-version: ["3.11"]
        test-config:
          - name: "minimal"
            ml: false
            competitive: false
            pytorch: false
            description: "Core datason performance"
          - name: "with-ml"
            ml: true
            competitive: false
            pytorch: true
            description: "ML library integration performance"
          - name: "competitive"
            ml: true
            competitive: true
            pytorch: false
            description: "Competitive analysis vs other libraries"

    name: "ðŸ”¬ ${{ matrix.test-config.name }} (Python ${{ matrix.python-version }})"

    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4

    - name: ðŸ Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: ðŸ’¾ Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: comprehensive-performance-${{ runner.os }}-py3.11-${{ matrix.test-config.name }}-${{ hashFiles('**/pyproject.toml', '**/requirements-benchmarking.txt') }}
        restore-keys: |
          comprehensive-performance-${{ runner.os }}-py3.11-${{ matrix.test-config.name }}-
          comprehensive-performance-${{ runner.os }}-py3.11-

    - name: ðŸ“¦ Install base package and dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .

    - name: ðŸ“¦ Install ML libraries (if enabled)
      if: matrix.test-config.ml == true
      run: |
        # Install in smaller chunks to avoid timeouts
        echo "Installing numpy and pandas..."
        pip install numpy pandas scikit-learn --timeout=300
        echo "âœ… ML libraries installed successfully"

    - name: ðŸ“¦ Install competitive libraries (if enabled)
      if: matrix.test-config.competitive == true
      run: |
        # Install competitive libraries with timeout protection
        echo "Installing competitive libraries..."
        pip install orjson ujson msgpack --timeout=300
        echo "âœ… Competitive libraries installed successfully"

    - name: ðŸ“¦ Install PyTorch (if enabled)
      if: matrix.test-config.pytorch == true
      run: |
        # Install PyTorch with CPU-only to save time and space
        echo "Installing PyTorch CPU..."
        pip install torch --index-url https://download.pytorch.org/whl/cpu --timeout=600
        echo "âœ… PyTorch CPU installed successfully"

    - name: ðŸ” Show installed packages
      run: pip list

    - name: ðŸš€ Run comprehensive performance tests
      run: |
        cd benchmarks
        python comprehensive_performance_suite.py
      env:
        TEST_CONFIG: ${{ matrix.test-config.name }}

    - name: ðŸ“Š Generate performance report
      run: |
        cd benchmarks
        python -c "
        import json
        import glob
        import os
        from datetime import datetime

        # Find the most recent comprehensive results
        result_files = sorted(glob.glob('comprehensive_performance_*.json'))
        if not result_files:
            print('No comprehensive results found')
            exit(0)

        with open(result_files[-1], 'r') as f:
            results = json.load(f)

        # Create GitHub Actions summary
        with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
            f.write('# ðŸ”¬ Comprehensive Performance Analysis\n\n')
            f.write(f'**Configuration**: ${{ matrix.test-config.description }}\n\n')

            # ML Libraries status
            ml_libs = results['metadata'].get('ml_libraries', {})
            f.write('## ðŸ“š Available Libraries\n\n')
            if ml_libs:
                for lib, version in ml_libs.items():
                    f.write(f'- **{lib}**: {version}\n')
            else:
                f.write('- No ML libraries available\n')

            # Competitive libraries
            comp_libs = results['metadata'].get('competitive_libs', {})
            if comp_libs:
                f.write('\n## âš”ï¸ Competitive Libraries\n\n')
                for lib, version in comp_libs.items():
                    f.write(f'- **{lib}**: {version}\n')

            # Competitive analysis
            comp_analysis = results.get('competitive_analysis', {})
            competitors = comp_analysis.get('datason_vs_competitors', {})

            if competitors:
                f.write('\n## ðŸ“Š Performance vs Competitors\n\n')
                f.write('| Library | Avg Slowdown | Best Case | Worst Case | Tests |\n')
                f.write('|---------|--------------|-----------|------------|-------|\n')

                for competitor, metrics in competitors.items():
                    f.write(f'| {competitor} | {metrics["average_slowdown_factor"]:.1f}x | {metrics["best_case"]:.1f}x | {metrics["worst_case"]:.1f}x | {metrics["test_count"]} |\n')

            # Recommendations
            recommendations = comp_analysis.get('recommendations', [])
            if recommendations:
                f.write('\n## ðŸ’¡ Recommendations\n\n')
                for rec in recommendations:
                    f.write(f'- {rec}\n')

            # Test coverage
            f.write('\n## ðŸ”¬ Test Coverage\n\n')
            ml_tests = len(results.get('ml_benchmarks', {}))
            complex_tests = len(results.get('complex_data_benchmarks', {}))
            f.write(f'- **ML Integration Tests**: {ml_tests}\n')
            f.write(f'- **Complex Data Tests**: {complex_tests}\n')
            f.write(f'- **Total Scenarios**: {ml_tests + complex_tests}\n')
        "

    - name: ðŸ’¾ Upload comprehensive results
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-results-${{ matrix.test-config.name }}-${{ github.run_id }}
        path: |
          benchmarks/comprehensive_performance_*.json
        retention-days: 180  # Keep longer for trend analysis

    - name: ðŸ“ˆ Create performance comparison comment (PR only)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          // Find comprehensive results
          const resultsDir = 'benchmarks/';
          const files = fs.readdirSync(resultsDir).filter(f => f.startsWith('comprehensive_performance_'));

          if (files.length === 0) {
            console.log('No comprehensive results found');
            return;
          }

          const resultsFile = path.join(resultsDir, files[files.length - 1]);
          const results = JSON.parse(fs.readFileSync(resultsFile, 'utf8'));

          const competitors = results.competitive_analysis?.datason_vs_competitors || {};

          let comment = `## ðŸ”¬ Comprehensive Performance Analysis (${{ matrix.test-config.name }})\n\n`;

          if (Object.keys(competitors).length > 0) {
            comment += `### Performance vs Competitors\n\n`;
            comment += `| Library | Average Slowdown | Range |\n`;
            comment += `|---------|------------------|-------|\n`;

            for (const [lib, metrics] of Object.entries(competitors)) {
              comment += `| ${lib} | ${metrics.average_slowdown_factor.toFixed(1)}x | ${metrics.best_case.toFixed(1)}x - ${metrics.worst_case.toFixed(1)}x |\n`;
            }

            const recommendations = results.competitive_analysis?.recommendations || [];
            if (recommendations.length > 0) {
              comment += `\n### Recommendations\n\n`;
              recommendations.forEach(rec => {
                comment += `- ${rec}\n`;
              });
            }
          } else {
            comment += `No competitive comparison data available for this configuration.\n`;
          }

          // Only comment on PRs
          if (context.eventName === 'pull_request') {
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  # Aggregate results across all configurations
  aggregate-results:
    runs-on: ubuntu-latest
    needs: comprehensive-performance
    if: always()

    steps:
    - name: ðŸ“¥ Download all results
      uses: actions/download-artifact@v4
      with:
        path: results/
        pattern: comprehensive-results-*

    - name: ðŸ“Š Aggregate and analyze results
      run: |
        python -c "
        import json
        import os
        import glob
        from datetime import datetime

        # Collect all result files
        all_results = {}
        for config_dir in glob.glob('results/comprehensive-results-*'):
            config_name = os.path.basename(config_dir).replace('comprehensive-results-', '').split('-')[0]
            result_files = glob.glob(os.path.join(config_dir, 'comprehensive_performance_*.json'))

            if result_files:
                with open(result_files[0], 'r') as f:
                    all_results[config_name] = json.load(f)

        # Create aggregated summary
        summary = {
            'timestamp': datetime.now().isoformat(),
            'configurations': list(all_results.keys()),
            'competitive_summary': {},
            'ml_library_coverage': {},
        }

        # Aggregate competitive data
        for config, results in all_results.items():
            competitors = results.get('competitive_analysis', {}).get('datason_vs_competitors', {})
            for lib, metrics in competitors.items():
                if lib not in summary['competitive_summary']:
                    summary['competitive_summary'][lib] = []
                # Safely access average_slowdown_factor with fallback
                slowdown = metrics.get('average_slowdown_factor', 0)
                if slowdown > 0:  # Only include valid slowdown factors
                    summary['competitive_summary'][lib].append({
                        'config': config,
                        'slowdown': slowdown
                    })

        # Save aggregated results
        with open('aggregated_comprehensive_results.json', 'w') as f:
            json.dump(summary, f, indent=2)

        print('ðŸ“Š Aggregated Results Summary:')
        print(f'Configurations tested: {len(all_results)}')

        if summary['competitive_summary']:
            for lib, configs in summary['competitive_summary'].items():
                if configs:  # Only process libraries with valid data
                    avg_slowdown = sum(c['slowdown'] for c in configs) / len(configs)
                    print(f'{lib}: {avg_slowdown:.1f}x slower (average across {len(configs)} configs)')
        else:
            print('No competitive analysis data found across configurations')
        "

    - name: ðŸ’¾ Upload aggregated results
      uses: actions/upload-artifact@v4
      with:
        name: aggregated-comprehensive-results-${{ github.run_id }}
        path: aggregated_comprehensive_results.json
        retention-days: 365  # Keep for yearly analysis
