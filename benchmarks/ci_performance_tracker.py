#!/usr/bin/env python3
"""
CI Performance Tracker for datason
==================================

This script provides a stable, CI-friendly performance testing suite designed for:
1. Consistent, reproducible performance measurements
2. Historical data tracking
3. Regression detection
4. Progress measurement over incremental improvements

The benchmarks are designed to be stable (minimal variance) and representative
of real-world usage patterns.
"""

import json
import os
import platform
import sys
import time
import uuid
from datetime import datetime, timezone
from decimal import Decimal
from statistics import mean, stdev
from typing import Any, Dict, Optional

import datason


class PerformanceTracker:
    """Tracks performance metrics with historical context."""

    def __init__(self, results_dir: str = "benchmarks/results"):
        self.results_dir = results_dir
        self.metadata = self._gather_metadata()
        os.makedirs(results_dir, exist_ok=True)

    def _gather_metadata(self) -> Dict[str, Any]:
        """Gather consistent environment metadata."""
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "python_version": sys.version,
            "platform": platform.platform(),
            "datason_version": getattr(datason, "__version__", "unknown"),
            "git_commit": os.environ.get("GITHUB_SHA", "unknown"),
            "git_ref": os.environ.get("GITHUB_REF", "unknown"),
            "ci_run_id": os.environ.get("GITHUB_RUN_ID", "local"),
            "runner": os.environ.get("RUNNER_OS", "local"),
        }

    def benchmark_function(self, func, iterations: int = 10, warmup: int = 2) -> Dict[str, float]:
        """Run a function multiple times and return statistics."""
        # Warmup runs to stabilize performance
        for _ in range(warmup):
            func()

        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            func()
            end = time.perf_counter()
            times.append(end - start)

        return {
            "mean": mean(times),
            "min": min(times),
            "max": max(times),
            "std": stdev(times) if len(times) > 1 else 0.0,
            "median": sorted(times)[len(times) // 2],
            "iterations": iterations,
        }


class StableBenchmarkSuite:
    """Stable benchmark suite designed for CI tracking."""

    def __init__(self):
        self.tracker = PerformanceTracker()

    def create_stable_test_data(self) -> Dict[str, Any]:
        """Create deterministic test data for consistent benchmarking."""
        # Use fixed seed/deterministic data to ensure consistency across runs
        return {
            "api_response": {
                "status": "success",
                "timestamp": datetime(2024, 1, 1, 12, 0, 0, tzinfo=timezone.utc),
                "request_id": "12345678-1234-5678-9012-123456789012",  # Fixed UUID
                "data": [
                    {
                        "id": i,
                        "name": f"Item {i:03d}",
                        "price": Decimal(f"{19.99 + i * 0.50:.2f}"),
                        "created_at": datetime(2024, 1, 1, 12, min(i, 59), 0, tzinfo=timezone.utc),
                        "tags": [f"tag{j}" for j in range(i % 3 + 1)],
                        "active": i % 2 == 0,
                    }
                    for i in range(100)  # Fixed size for consistency
                ],
            },
            "config_data": {
                "app_name": "TestApp",
                "version": "1.0.0",
                "database": {
                    "host": "localhost",
                    "port": 5432,
                    "timeout": 30.0,
                },
                "created": datetime(2024, 1, 1, 12, 0, 0, tzinfo=timezone.utc),
                "feature_flags": [f"feature_{i}" for i in range(10)],
            },
            "simple_types": {
                "string": "hello world",
                "integer": 42,
                "float": 3.14159,
                "boolean": True,
                "null": None,
                "list": [1, 2, 3, 4, 5],
                "nested": {"a": {"b": {"c": "deep"}}},
            },
        }

    def benchmark_serialization(self) -> Dict[str, Any]:
        """Benchmark serialization performance."""
        test_data = self.create_stable_test_data()
        results = {}

        for name, data in test_data.items():
            print(f"📊 Benchmarking serialization: {name}")

            # Standard datason
            def serialize_standard(data=data):
                return datason.serialize(data)

            # Performance config
            perf_config = datason.get_performance_config()

            def serialize_perf(data=data, config=perf_config):
                return datason.serialize(data, config=config)

            results[name] = {
                "standard": self.tracker.benchmark_function(serialize_standard),
                "performance_config": self.tracker.benchmark_function(serialize_perf),
            }

            # Calculate improvement
            std_time = results[name]["standard"]["mean"]
            perf_time = results[name]["performance_config"]["mean"]
            improvement = ((std_time - perf_time) / std_time) * 100
            results[name]["performance_improvement_pct"] = improvement

            print(f"  Standard: {std_time * 1000:.2f}ms")
            print(f"  Performance: {perf_time * 1000:.2f}ms")
            print(f"  Improvement: {improvement:.1f}%")

        return results

    def benchmark_deserialization(self) -> Dict[str, Any]:
        """Benchmark deserialization performance."""
        test_data = self.create_stable_test_data()
        results = {}

        for name, data in test_data.items():
            print(f"📊 Benchmarking deserialization: {name}")

            # Serialize data first
            serialized = datason.serialize(data)

            # Standard deserialization
            def deserialize_standard(serialized=serialized):
                return datason.deserialize(serialized)

            results[name] = {
                "standard": self.tracker.benchmark_function(deserialize_standard),
            }

            print(f"  Standard: {results[name]['standard']['mean'] * 1000:.2f}ms")

        return results

    def benchmark_type_detection_overhead(self) -> Dict[str, Any]:
        """Benchmark specific type detection patterns."""
        results = {}

        test_cases = {
            "simple_dict": {"a": 1, "b": "hello", "c": True},
            "datetime_objects": [datetime.now(timezone.utc) for _ in range(50)],
            "uuid_objects": [str(uuid.UUID(int=i)) for i in range(50)],
            "decimal_objects": [Decimal(f"{i}.99") for i in range(50)],
            "mixed_list": [
                {"id": i, "uuid": str(uuid.UUID(int=i)), "timestamp": datetime.now(timezone.utc)} for i in range(25)
            ],
        }

        # Benchmark each test case
        for name, data in test_cases.items():

            def serialize_typed(data=data):
                return datason.serialize(data)

            results[name] = self.tracker.benchmark_function(serialize_typed)
            print(f"  Time: {results[name]['mean'] * 1000:.2f}ms")

        return results

    def run_all_benchmarks(self) -> Dict[str, Any]:
        """Run complete benchmark suite."""
        print("🚀 Starting CI Performance Benchmark Suite")
        print("=" * 60)

        results = {
            "metadata": self.tracker.metadata,
            "benchmarks": {
                "serialization": self.benchmark_serialization(),
                "deserialization": self.benchmark_deserialization(),
                "type_detection": self.benchmark_type_detection_overhead(),
            },
        }

        # Save results
        filename = f"performance_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(self.tracker.results_dir, filename)

        with open(filepath, "w") as f:
            json.dump(results, f, indent=2, default=str)

        print(f"\n💾 Results saved to: {filepath}")

        # Also save as latest.json for easy CI access
        latest_path = os.path.join(self.tracker.results_dir, "latest.json")
        with open(latest_path, "w") as f:
            json.dump(results, f, indent=2, default=str)

        return results


class PerformanceRegression:
    """Detect performance regressions by comparing with baseline."""

    def __init__(self, results_dir: str = "benchmarks/results"):
        self.results_dir = results_dir

    def load_baseline(self) -> Optional[Dict[str, Any]]:
        """Load baseline performance data."""
        baseline_path = os.path.join(self.results_dir, "baseline.json")
        if os.path.exists(baseline_path):
            with open(baseline_path) as f:
                return json.load(f)
        return None

    def save_baseline(self, results: Dict[str, Any]) -> None:
        """Save current results as new baseline."""
        baseline_path = os.path.join(self.results_dir, "baseline.json")
        with open(baseline_path, "w") as f:
            json.dump(results, f, indent=2, default=str)
        print(f"📋 Saved new baseline: {baseline_path}")

    def compare_with_baseline(self, current_results: Dict[str, Any]) -> Dict[str, Any]:
        """Compare current results with baseline and detect regressions."""
        # Try CI-specific baseline first, fallback to general baseline
        ci_baseline_path = os.path.join(self.results_dir, "baseline_ci.json")
        general_baseline_path = os.path.join(self.results_dir, "baseline.json")

        baseline = None
        baseline_source = "none"

        # Prefer CI baseline when running in CI
        if os.environ.get("CI_ENVIRONMENT"):
            if os.path.exists(ci_baseline_path):
                with open(ci_baseline_path) as f:
                    baseline = json.load(f)
                baseline_source = "ci"
            elif os.path.exists(general_baseline_path):
                with open(general_baseline_path) as f:
                    baseline = json.load(f)
                baseline_source = "local"
        else:
            # Local development - use general baseline
            if os.path.exists(general_baseline_path):
                with open(general_baseline_path) as f:
                    baseline = json.load(f)
                baseline_source = "local"

        if not baseline:
            print("⚠️  No baseline found. Saving current results as baseline.")
            baseline_path = ci_baseline_path if os.environ.get("CI_ENVIRONMENT") else general_baseline_path

            with open(baseline_path, "w") as f:
                json.dump(current_results, f, indent=2, default=str)
            print(f"📋 Saved new baseline: {baseline_path}")
            return {"status": "baseline_created"}

        # Environment-aware threshold
        threshold = float(os.environ.get("PERFORMANCE_REGRESSION_THRESHOLD", "5"))

        # Adjust threshold based on environment mismatch
        environment_mismatch = False
        if baseline_source == "local" and os.environ.get("CI_ENVIRONMENT"):
            environment_mismatch = True
            threshold = max(threshold, 25)  # Minimum 25% threshold for environment mismatch
            print(f"⚠️  Environment mismatch: Using local baseline in CI (threshold: {threshold}%)")
        elif baseline_source == "ci" and not os.environ.get("CI_ENVIRONMENT"):
            environment_mismatch = True
            threshold = max(threshold, 25)
            print(f"⚠️  Environment mismatch: Using CI baseline locally (threshold: {threshold}%)")

        comparison = {
            "status": "compared",
            "baseline_metadata": baseline.get("metadata", {}),
            "current_metadata": current_results.get("metadata", {}),
            "baseline_source": baseline_source,
            "environment_mismatch": environment_mismatch,
            "threshold_used": threshold,
            "performance_changes": {},
            "regressions": [],
            "improvements": [],
        }

        # Compare each benchmark
        for category in ["serialization", "deserialization", "type_detection"]:
            if category not in baseline.get("benchmarks", {}):
                continue

            baseline_bench = baseline["benchmarks"][category]
            current_bench = current_results["benchmarks"][category]

            for test_name in baseline_bench:
                if test_name not in current_bench:
                    continue

                # Compare mean times
                if category in ["serialization", "deserialization"]:
                    # These have nested structure (standard/performance_config)
                    for config in ["standard", "performance_config"]:
                        if config in baseline_bench[test_name] and config in current_bench[test_name]:
                            baseline_time = baseline_bench[test_name][config]["mean"]
                            current_time = current_bench[test_name][config]["mean"]
                            change_pct = ((current_time - baseline_time) / baseline_time) * 100

                            key = f"{category}.{test_name}.{config}"
                            comparison["performance_changes"][key] = {
                                "baseline_ms": baseline_time * 1000,
                                "current_ms": current_time * 1000,
                                "change_pct": change_pct,
                            }

                            # Detect significant changes with environment-aware threshold
                            if change_pct > threshold:
                                comparison["regressions"].append(
                                    {
                                        "test": key,
                                        "change_pct": change_pct,
                                        "baseline_ms": baseline_time * 1000,
                                        "current_ms": current_time * 1000,
                                    }
                                )
                            elif change_pct < -threshold:
                                comparison["improvements"].append(
                                    {
                                        "test": key,
                                        "change_pct": change_pct,
                                        "baseline_ms": baseline_time * 1000,
                                        "current_ms": current_time * 1000,
                                    }
                                )
                else:
                    # Type detection has flat structure
                    baseline_time = baseline_bench[test_name]["mean"]
                    current_time = current_bench[test_name]["mean"]
                    change_pct = ((current_time - baseline_time) / baseline_time) * 100

                    key = f"{category}.{test_name}"
                    comparison["performance_changes"][key] = {
                        "baseline_ms": baseline_time * 1000,
                        "current_ms": current_time * 1000,
                        "change_pct": change_pct,
                    }

                    if change_pct > threshold:
                        comparison["regressions"].append(
                            {
                                "test": key,
                                "change_pct": change_pct,
                                "baseline_ms": baseline_time * 1000,
                                "current_ms": current_time * 1000,
                            }
                        )
                    elif change_pct < -threshold:
                        comparison["improvements"].append(
                            {
                                "test": key,
                                "change_pct": change_pct,
                                "baseline_ms": baseline_time * 1000,
                                "current_ms": current_time * 1000,
                            }
                        )

        return comparison


def main():
    """Main function for CI execution."""
    print("🎯 Datason CI Performance Tracker")
    print("=" * 50)

    # Environment and threshold info
    threshold = float(os.environ.get("PERFORMANCE_REGRESSION_THRESHOLD", "5"))
    ci_env = bool(os.environ.get("CI_ENVIRONMENT"))

    if ci_env:
        print(f"🌐 Running in CI environment (threshold: {threshold}%)")
    else:
        print(f"💻 Running locally (threshold: {threshold}%)")

    # Run benchmarks
    suite = StableBenchmarkSuite()
    results = suite.run_all_benchmarks()

    # Check for regressions
    regression_checker = PerformanceRegression()
    comparison = regression_checker.compare_with_baseline(results)

    # Print summary
    print("\n📊 Performance Summary:")
    print("-" * 30)

    if comparison["status"] == "baseline_created":
        print("✅ Baseline created successfully")
        return 0

    print(f"✅ Compared with baseline from {comparison['baseline_metadata'].get('timestamp', 'unknown')}")

    if comparison.get("environment_mismatch"):
        print(f"⚠️  Environment mismatch detected (using {comparison['threshold_used']}% threshold)")
    else:
        print(f"📏 Using {comparison['threshold_used']}% threshold for regression detection")

    if comparison["regressions"]:
        print(f"\n🔴 Performance Regressions ({len(comparison['regressions'])}):")
        for reg in comparison["regressions"]:
            severity = "🔥 MAJOR" if reg["change_pct"] > 100 else "⚠️  minor"
            print(
                f"  {reg['test']}: {reg['change_pct']:+.1f}% ({reg['current_ms']:.2f}ms vs {reg['baseline_ms']:.2f}ms) {severity}"
            )

    if comparison["improvements"]:
        print(f"\n🟢 Performance Improvements ({len(comparison['improvements'])}):")
        for imp in comparison["improvements"]:
            print(
                f"  {imp['test']}: {imp['change_pct']:+.1f}% ({imp['current_ms']:.2f}ms vs {imp['baseline_ms']:.2f}ms)"
            )

    if not comparison["regressions"] and not comparison["improvements"]:
        print("🟡 No significant performance changes detected")

    # Provide context about environment impact
    if comparison.get("environment_mismatch"):
        print("\nℹ️  Note: Results compared across different environments")
        print("   Large differences may be due to hardware/OS differences rather than code changes")

    # Save comparison results
    comparison_path = os.path.join("benchmarks/results", "latest_comparison.json")
    with open(comparison_path, "w") as f:
        json.dump(comparison, f, indent=2, default=str)

    # Return appropriate exit code - but only fail for major regressions in CI
    if comparison["regressions"]:
        major_regressions = [r for r in comparison["regressions"] if r["change_pct"] > 100]

        if ci_env and major_regressions and not comparison.get("environment_mismatch"):
            # Only fail in CI for major regressions when environments match
            print(f"\n❌ Found {len(major_regressions)} major performance regressions in CI")
            return 1
        else:
            if comparison.get("environment_mismatch"):
                print(
                    f"\n⚠️  Found {len(comparison['regressions'])} regressions, but likely due to environment differences"
                )
            else:
                print(f"\n⚠️  Found {len(comparison['regressions'])} minor performance regressions")
            return 0
    else:
        print("\n✅ No performance regressions detected")
        return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
